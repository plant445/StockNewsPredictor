{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:35:40.230257Z",
     "start_time": "2024-11-24T16:35:37.696493Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:35:59.765293Z",
     "start_time": "2024-11-24T16:35:43.389484Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load datasets\n",
    "news_data = pd.read_csv(\"news.csv\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Combine Top1-Top25 into a single string\n",
    "news_data['combined_text'] = news_data.iloc[:, 2:].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "\n",
    "# Tokenize headlines\n",
    "news_data['tokens'] = news_data['combined_text'].apply(\n",
    "    lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    ")\n",
    "\n",
    "labels = news_data['Label'].values  \n",
    "# Ensure this column is properly set for stock movement\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:43:46.424932Z",
     "start_time": "2024-11-24T16:43:46.421830Z"
    }
   },
   "source": [
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # Text embeddings\n",
    "        bert_output = self.bert(**tokens).pooler_output\n",
    "        return torch.sigmoid(self.fc(bert_output))\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T16:45:00.665221Z",
     "start_time": "2024-11-24T16:43:47.998972Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, AdamW\n",
    "import torch\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Define Dataset Class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news, labels):\n",
    "        self.news = news\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.news[idx], self.labels[idx]\n",
    "\n",
    "# Prepare tokenized text data\n",
    "train_news = news_data['tokens'].tolist()  # This contains a list of dictionaries\n",
    "train_labels = news_data['Label'].values\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = NewsDataset(train_news, train_labels)\n",
    "\n",
    "# Define Collate Function\n",
    "def collate_fn(batch):\n",
    "    news = {key: torch.cat([b[0][key] for b in batch], dim=0) for key in batch[0][0].keys()}  # Tokenized text\n",
    "    labels = torch.stack([b[1] for b in batch])  # Labels\n",
    "    return news, labels\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define Model\n",
    "model = StockPredictor()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    batch_index = 0\n",
    "    for batch in train_loader:\n",
    "        news, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(news).squeeze()\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: {param.grad.norm()}\")\n",
    "        optimizer.step()\n",
    "        batch_index += 1\n",
    "        print(f\"Epoch: {epoch+1}, Batch: {batch_index}, Loss: {loss.item()}\")\n",
    "            "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "bert.embeddings.word_embeddings.weight: 0.022414805367588997\n",
      "bert.embeddings.position_embeddings.weight: 0.02108394168317318\n",
      "bert.embeddings.token_type_embeddings.weight: 0.04883246123790741\n",
      "bert.embeddings.LayerNorm.weight: 0.004237204324454069\n",
      "bert.embeddings.LayerNorm.bias: 0.003059790935367346\n",
      "bert.encoder.layer.0.attention.self.query.weight: 0.004916129168123007\n",
      "bert.encoder.layer.0.attention.self.query.bias: 0.00030045295716263354\n",
      "bert.encoder.layer.0.attention.self.key.weight: 0.003913254477083683\n",
      "bert.encoder.layer.0.attention.self.key.bias: 8.423366171239621e-11\n",
      "bert.encoder.layer.0.attention.self.value.weight: 0.013453829102218151\n",
      "bert.encoder.layer.0.attention.self.value.bias: 0.0025498303584754467\n",
      "bert.encoder.layer.0.attention.output.dense.weight: 0.011669205501675606\n",
      "bert.encoder.layer.0.attention.output.dense.bias: 0.002466656733304262\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.001222953200340271\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0017654128605499864\n",
      "bert.encoder.layer.0.intermediate.dense.weight: 0.028069602325558662\n",
      "bert.encoder.layer.0.intermediate.dense.bias: 0.0008121076389215887\n",
      "bert.encoder.layer.0.output.dense.weight: 0.012586262077093124\n",
      "bert.encoder.layer.0.output.dense.bias: 0.0017486887518316507\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: 0.004688063636422157\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: 0.0036692151334136724\n",
      "bert.encoder.layer.1.attention.self.query.weight: 0.006551092490553856\n",
      "bert.encoder.layer.1.attention.self.query.bias: 0.0005088653997518122\n",
      "bert.encoder.layer.1.attention.self.key.weight: 0.005879433825612068\n",
      "bert.encoder.layer.1.attention.self.key.bias: 1.368319069161572e-10\n",
      "bert.encoder.layer.1.attention.self.value.weight: 0.023338111117482185\n",
      "bert.encoder.layer.1.attention.self.value.bias: 0.00291902432218194\n",
      "bert.encoder.layer.1.attention.output.dense.weight: 0.014980136416852474\n",
      "bert.encoder.layer.1.attention.output.dense.bias: 0.002996342722326517\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0016216348158195615\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.0023463552352041006\n",
      "bert.encoder.layer.1.intermediate.dense.weight: 0.032624855637550354\n",
      "bert.encoder.layer.1.intermediate.dense.bias: 0.0011173454113304615\n",
      "bert.encoder.layer.1.output.dense.weight: 0.014227991923689842\n",
      "bert.encoder.layer.1.output.dense.bias: 0.002174879889935255\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: 0.0027424192521721125\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: 0.003438073443248868\n",
      "bert.encoder.layer.2.attention.self.query.weight: 0.007712482009083033\n",
      "bert.encoder.layer.2.attention.self.query.bias: 0.0005294255679473281\n",
      "bert.encoder.layer.2.attention.self.key.weight: 0.0072162700816988945\n",
      "bert.encoder.layer.2.attention.self.key.bias: 1.877865501986875e-10\n",
      "bert.encoder.layer.2.attention.self.value.weight: 0.02679453045129776\n",
      "bert.encoder.layer.2.attention.self.value.bias: 0.0026581278070807457\n",
      "bert.encoder.layer.2.attention.output.dense.weight: 0.017207929864525795\n",
      "bert.encoder.layer.2.attention.output.dense.bias: 0.0031392963137477636\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.001603280077688396\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0027198309544473886\n",
      "bert.encoder.layer.2.intermediate.dense.weight: 0.02939874865114689\n",
      "bert.encoder.layer.2.intermediate.dense.bias: 0.0010067024268209934\n",
      "bert.encoder.layer.2.output.dense.weight: 0.014625024981796741\n",
      "bert.encoder.layer.2.output.dense.bias: 0.0024780824314802885\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: 0.0035910739097744226\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: 0.0038004256784915924\n",
      "bert.encoder.layer.3.attention.self.query.weight: 0.006460677832365036\n",
      "bert.encoder.layer.3.attention.self.query.bias: 0.00034461726318113506\n",
      "bert.encoder.layer.3.attention.self.key.weight: 0.00587273295968771\n",
      "bert.encoder.layer.3.attention.self.key.bias: 1.3441334156816254e-10\n",
      "bert.encoder.layer.3.attention.self.value.weight: 0.02538342960178852\n",
      "bert.encoder.layer.3.attention.self.value.bias: 0.002619032748043537\n",
      "bert.encoder.layer.3.attention.output.dense.weight: 0.019825145602226257\n",
      "bert.encoder.layer.3.attention.output.dense.bias: 0.003601335221901536\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.001661133486777544\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.0027718525379896164\n",
      "bert.encoder.layer.3.intermediate.dense.weight: 0.033502209931612015\n",
      "bert.encoder.layer.3.intermediate.dense.bias: 0.0010729157365858555\n",
      "bert.encoder.layer.3.output.dense.weight: 0.016231078654527664\n",
      "bert.encoder.layer.3.output.dense.bias: 0.0025205763522535563\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: 0.007111788727343082\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: 0.004015928599983454\n",
      "bert.encoder.layer.4.attention.self.query.weight: 0.01024661771953106\n",
      "bert.encoder.layer.4.attention.self.query.bias: 0.0006104625063017011\n",
      "bert.encoder.layer.4.attention.self.key.weight: 0.009528189897537231\n",
      "bert.encoder.layer.4.attention.self.key.bias: 1.9773035986325738e-10\n",
      "bert.encoder.layer.4.attention.self.value.weight: 0.026200665161013603\n",
      "bert.encoder.layer.4.attention.self.value.bias: 0.002991701476275921\n",
      "bert.encoder.layer.4.attention.output.dense.weight: 0.01543023157864809\n",
      "bert.encoder.layer.4.attention.output.dense.bias: 0.002938703401014209\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.002013961784541607\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.002684027887880802\n",
      "bert.encoder.layer.4.intermediate.dense.weight: 0.04252411797642708\n",
      "bert.encoder.layer.4.intermediate.dense.bias: 0.0020000149961560965\n",
      "bert.encoder.layer.4.output.dense.weight: 0.016985932365059853\n",
      "bert.encoder.layer.4.output.dense.bias: 0.003004401456564665\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: 0.010460599325597286\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: 0.004841998219490051\n",
      "bert.encoder.layer.5.attention.self.query.weight: 0.0074239918030798435\n",
      "bert.encoder.layer.5.attention.self.query.bias: 0.00043274948257021606\n",
      "bert.encoder.layer.5.attention.self.key.weight: 0.007029256783425808\n",
      "bert.encoder.layer.5.attention.self.key.bias: 1.1655082454709031e-10\n",
      "bert.encoder.layer.5.attention.self.value.weight: 0.027609428390860558\n",
      "bert.encoder.layer.5.attention.self.value.bias: 0.0030524597968906164\n",
      "bert.encoder.layer.5.attention.output.dense.weight: 0.015268032439053059\n",
      "bert.encoder.layer.5.attention.output.dense.bias: 0.003485599299892783\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0025729332119226456\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.00319708907045424\n",
      "bert.encoder.layer.5.intermediate.dense.weight: 0.027280017733573914\n",
      "bert.encoder.layer.5.intermediate.dense.bias: 0.0010021808557212353\n",
      "bert.encoder.layer.5.output.dense.weight: 0.014832420274615288\n",
      "bert.encoder.layer.5.output.dense.bias: 0.0028267200104892254\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: 0.003970608115196228\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: 0.004275992512702942\n",
      "bert.encoder.layer.6.attention.self.query.weight: 0.006889736279845238\n",
      "bert.encoder.layer.6.attention.self.query.bias: 0.00038368540117517114\n",
      "bert.encoder.layer.6.attention.self.key.weight: 0.00671007763594389\n",
      "bert.encoder.layer.6.attention.self.key.bias: 1.2548390104782925e-10\n",
      "bert.encoder.layer.6.attention.self.value.weight: 0.02482673153281212\n",
      "bert.encoder.layer.6.attention.self.value.bias: 0.002651266288012266\n",
      "bert.encoder.layer.6.attention.output.dense.weight: 0.015298379585146904\n",
      "bert.encoder.layer.6.attention.output.dense.bias: 0.0032124340068548918\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.002595855388790369\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.003079448128119111\n",
      "bert.encoder.layer.6.intermediate.dense.weight: 0.027139123529195786\n",
      "bert.encoder.layer.6.intermediate.dense.bias: 0.0010212707566097379\n",
      "bert.encoder.layer.6.output.dense.weight: 0.014960726723074913\n",
      "bert.encoder.layer.6.output.dense.bias: 0.0027683766093105078\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: 0.004233106970787048\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: 0.004103219602257013\n",
      "bert.encoder.layer.7.attention.self.query.weight: 0.004739082418382168\n",
      "bert.encoder.layer.7.attention.self.query.bias: 0.00026135967345908284\n",
      "bert.encoder.layer.7.attention.self.key.weight: 0.004584548994898796\n",
      "bert.encoder.layer.7.attention.self.key.bias: 7.396430690143063e-11\n",
      "bert.encoder.layer.7.attention.self.value.weight: 0.02302662283182144\n",
      "bert.encoder.layer.7.attention.self.value.bias: 0.0023042531684041023\n",
      "bert.encoder.layer.7.attention.output.dense.weight: 0.013115608133375645\n",
      "bert.encoder.layer.7.attention.output.dense.bias: 0.003301873803138733\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.0028915542643517256\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.0031073237769305706\n",
      "bert.encoder.layer.7.intermediate.dense.weight: 0.029006941244006157\n",
      "bert.encoder.layer.7.intermediate.dense.bias: 0.0010448425309732556\n",
      "bert.encoder.layer.7.output.dense.weight: 0.016496602445840836\n",
      "bert.encoder.layer.7.output.dense.bias: 0.0030436129309237003\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: 0.0031446132343262434\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: 0.004870546516031027\n",
      "bert.encoder.layer.8.attention.self.query.weight: 0.007375070825219154\n",
      "bert.encoder.layer.8.attention.self.query.bias: 0.00039780233055353165\n",
      "bert.encoder.layer.8.attention.self.key.weight: 0.007257099263370037\n",
      "bert.encoder.layer.8.attention.self.key.bias: 1.1519821901950777e-10\n",
      "bert.encoder.layer.8.attention.self.value.weight: 0.027101345360279083\n",
      "bert.encoder.layer.8.attention.self.value.bias: 0.0032062435057014227\n",
      "bert.encoder.layer.8.attention.output.dense.weight: 0.01641963981091976\n",
      "bert.encoder.layer.8.attention.output.dense.bias: 0.0039491308853030205\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.0033486252650618553\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.0034336200915277004\n",
      "bert.encoder.layer.8.intermediate.dense.weight: 0.034247953444719315\n",
      "bert.encoder.layer.8.intermediate.dense.bias: 0.0010941766668111086\n",
      "bert.encoder.layer.8.output.dense.weight: 0.01726071909070015\n",
      "bert.encoder.layer.8.output.dense.bias: 0.00329493242315948\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: 0.0032091259490698576\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: 0.004895176738500595\n",
      "bert.encoder.layer.9.attention.self.query.weight: 0.007388461846858263\n",
      "bert.encoder.layer.9.attention.self.query.bias: 0.0004968890571035445\n",
      "bert.encoder.layer.9.attention.self.key.weight: 0.006866266950964928\n",
      "bert.encoder.layer.9.attention.self.key.bias: 1.1907504149366588e-10\n",
      "bert.encoder.layer.9.attention.self.value.weight: 0.02617807686328888\n",
      "bert.encoder.layer.9.attention.self.value.bias: 0.003331138053908944\n",
      "bert.encoder.layer.9.attention.output.dense.weight: 0.019766785204410553\n",
      "bert.encoder.layer.9.attention.output.dense.bias: 0.0035365731455385685\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.0027042734436690807\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.003276888048276305\n",
      "bert.encoder.layer.9.intermediate.dense.weight: 0.025333430618047714\n",
      "bert.encoder.layer.9.intermediate.dense.bias: 0.0011202808236703277\n",
      "bert.encoder.layer.9.output.dense.weight: 0.019462859258055687\n",
      "bert.encoder.layer.9.output.dense.bias: 0.003226055298000574\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: 0.0024533479008823633\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: 0.004833262879401445\n",
      "bert.encoder.layer.10.attention.self.query.weight: 0.008363351225852966\n",
      "bert.encoder.layer.10.attention.self.query.bias: 0.0004825264331884682\n",
      "bert.encoder.layer.10.attention.self.key.weight: 0.007643134333193302\n",
      "bert.encoder.layer.10.attention.self.key.bias: 1.342914668356343e-10\n",
      "bert.encoder.layer.10.attention.self.value.weight: 0.02882974222302437\n",
      "bert.encoder.layer.10.attention.self.value.bias: 0.00374029204249382\n",
      "bert.encoder.layer.10.attention.output.dense.weight: 0.019719954580068588\n",
      "bert.encoder.layer.10.attention.output.dense.bias: 0.0032780044712126255\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.0031866496428847313\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.003148135496303439\n",
      "bert.encoder.layer.10.intermediate.dense.weight: 0.03489093482494354\n",
      "bert.encoder.layer.10.intermediate.dense.bias: 0.0018427673494443297\n",
      "bert.encoder.layer.10.output.dense.weight: 0.01848365180194378\n",
      "bert.encoder.layer.10.output.dense.bias: 0.002926294691860676\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: 0.0027647449169307947\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: 0.004085790831595659\n",
      "bert.encoder.layer.11.attention.self.query.weight: 0.008603295311331749\n",
      "bert.encoder.layer.11.attention.self.query.bias: 0.0005456898943521082\n",
      "bert.encoder.layer.11.attention.self.key.weight: 0.008683190681040287\n",
      "bert.encoder.layer.11.attention.self.key.bias: 1.6206108677252473e-10\n",
      "bert.encoder.layer.11.attention.self.value.weight: 0.03826712816953659\n",
      "bert.encoder.layer.11.attention.self.value.bias: 0.003027578117325902\n",
      "bert.encoder.layer.11.attention.output.dense.weight: 0.019340116530656815\n",
      "bert.encoder.layer.11.attention.output.dense.bias: 0.0023659488651901484\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.0018185460940003395\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.0017334558069705963\n",
      "bert.encoder.layer.11.intermediate.dense.weight: 0.030006591230630875\n",
      "bert.encoder.layer.11.intermediate.dense.bias: 0.0009104856289923191\n",
      "bert.encoder.layer.11.output.dense.weight: 0.01820979453623295\n",
      "bert.encoder.layer.11.output.dense.bias: 0.0016085718525573611\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: 0.005265375599265099\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: 0.003526618704199791\n",
      "bert.pooler.dense.weight: 0.09188749641180038\n",
      "bert.pooler.dense.bias: 0.006382569670677185\n",
      "fc.0.weight: 0.20227959752082825\n",
      "fc.0.bias: 0.013955176807940006\n",
      "fc.2.weight: 0.05677208676934242\n",
      "fc.2.bias: 0.031103767454624176\n",
      "Epoch: 1, Batch: 1, Loss: 0.7250170111656189\n",
      "bert.embeddings.word_embeddings.weight: 0.015798252075910568\n",
      "bert.embeddings.position_embeddings.weight: 0.01498490385711193\n",
      "bert.embeddings.token_type_embeddings.weight: 0.03482731804251671\n",
      "bert.embeddings.LayerNorm.weight: 0.0017034845659509301\n",
      "bert.embeddings.LayerNorm.bias: 0.002109047258272767\n",
      "bert.encoder.layer.0.attention.self.query.weight: 0.0027248722035437822\n",
      "bert.encoder.layer.0.attention.self.query.bias: 0.00017384975217282772\n",
      "bert.encoder.layer.0.attention.self.key.weight: 0.0025075364392250776\n",
      "bert.encoder.layer.0.attention.self.key.bias: 7.005977742391423e-11\n",
      "bert.encoder.layer.0.attention.self.value.weight: 0.006869891658425331\n",
      "bert.encoder.layer.0.attention.self.value.bias: 0.0012863692827522755\n",
      "bert.encoder.layer.0.attention.output.dense.weight: 0.0076048593036830425\n",
      "bert.encoder.layer.0.attention.output.dense.bias: 0.0015273725148290396\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0007016271119937301\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0011300911428406835\n",
      "bert.encoder.layer.0.intermediate.dense.weight: 0.017811225727200508\n",
      "bert.encoder.layer.0.intermediate.dense.bias: 0.00044220327981747687\n",
      "bert.encoder.layer.0.output.dense.weight: 0.008250337094068527\n",
      "bert.encoder.layer.0.output.dense.bias: 0.0011485143331810832\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: 0.0031715601217001677\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: 0.0021313412580639124\n",
      "bert.encoder.layer.1.attention.self.query.weight: 0.0043638101778924465\n",
      "bert.encoder.layer.1.attention.self.query.bias: 0.0002218991139670834\n",
      "bert.encoder.layer.1.attention.self.key.weight: 0.004159088246524334\n",
      "bert.encoder.layer.1.attention.self.key.bias: 1.0444463349745803e-10\n",
      "bert.encoder.layer.1.attention.self.value.weight: 0.01241984497755766\n",
      "bert.encoder.layer.1.attention.self.value.bias: 0.0015592570416629314\n",
      "bert.encoder.layer.1.attention.output.dense.weight: 0.009148488752543926\n",
      "bert.encoder.layer.1.attention.output.dense.bias: 0.0018195138545706868\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0009866177570074797\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.001420420128852129\n",
      "bert.encoder.layer.1.intermediate.dense.weight: 0.020378250628709793\n",
      "bert.encoder.layer.1.intermediate.dense.bias: 0.0007801994797773659\n",
      "bert.encoder.layer.1.output.dense.weight: 0.00906308088451624\n",
      "bert.encoder.layer.1.output.dense.bias: 0.001500381389632821\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: 0.003422378795221448\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: 0.0022671539336442947\n",
      "bert.encoder.layer.2.attention.self.query.weight: 0.004601861350238323\n",
      "bert.encoder.layer.2.attention.self.query.bias: 0.0003223834792152047\n",
      "bert.encoder.layer.2.attention.self.key.weight: 0.004013696685433388\n",
      "bert.encoder.layer.2.attention.self.key.bias: 9.810398327037007e-11\n",
      "bert.encoder.layer.2.attention.self.value.weight: 0.017149370163679123\n",
      "bert.encoder.layer.2.attention.self.value.bias: 0.001622870098799467\n",
      "bert.encoder.layer.2.attention.output.dense.weight: 0.011736164800822735\n",
      "bert.encoder.layer.2.attention.output.dense.bias: 0.0020873895846307278\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0010955623583868146\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0018089624354615808\n",
      "bert.encoder.layer.2.intermediate.dense.weight: 0.01973014511168003\n",
      "bert.encoder.layer.2.intermediate.dense.bias: 0.0006705399136990309\n",
      "bert.encoder.layer.2.output.dense.weight: 0.01066860556602478\n",
      "bert.encoder.layer.2.output.dense.bias: 0.001831810106523335\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: 0.005849702749401331\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: 0.002825626404955983\n",
      "bert.encoder.layer.3.attention.self.query.weight: 0.005742367822676897\n",
      "bert.encoder.layer.3.attention.self.query.bias: 0.0004492784501053393\n",
      "bert.encoder.layer.3.attention.self.key.weight: 0.005143777467310429\n",
      "bert.encoder.layer.3.attention.self.key.bias: 1.0944369716048286e-10\n",
      "bert.encoder.layer.3.attention.self.value.weight: 0.018475979566574097\n",
      "bert.encoder.layer.3.attention.self.value.bias: 0.0020246077328920364\n",
      "bert.encoder.layer.3.attention.output.dense.weight: 0.012808338738977909\n",
      "bert.encoder.layer.3.attention.output.dense.bias: 0.002350086811929941\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0018625268712639809\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.002052160445600748\n",
      "bert.encoder.layer.3.intermediate.dense.weight: 0.03440818190574646\n",
      "bert.encoder.layer.3.intermediate.dense.bias: 0.0009494039113633335\n",
      "bert.encoder.layer.3.output.dense.weight: 0.0114244744181633\n",
      "bert.encoder.layer.3.output.dense.bias: 0.0019418532028794289\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: 0.0022452641278505325\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: 0.0030746019911020994\n",
      "bert.encoder.layer.4.attention.self.query.weight: 0.007440081797540188\n",
      "bert.encoder.layer.4.attention.self.query.bias: 0.0004765100893564522\n",
      "bert.encoder.layer.4.attention.self.key.weight: 0.007239185273647308\n",
      "bert.encoder.layer.4.attention.self.key.bias: 1.210477412749711e-10\n",
      "bert.encoder.layer.4.attention.self.value.weight: 0.017364537343382835\n",
      "bert.encoder.layer.4.attention.self.value.bias: 0.002003219211474061\n",
      "bert.encoder.layer.4.attention.output.dense.weight: 0.011190207675099373\n",
      "bert.encoder.layer.4.attention.output.dense.bias: 0.002129953121766448\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0011326937237754464\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.002020108513534069\n",
      "bert.encoder.layer.4.intermediate.dense.weight: 0.021261323243379593\n",
      "bert.encoder.layer.4.intermediate.dense.bias: 0.0008477496448904276\n",
      "bert.encoder.layer.4.output.dense.weight: 0.011147714219987392\n",
      "bert.encoder.layer.4.output.dense.bias: 0.0020440653897821903\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: 0.0022671259939670563\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: 0.003165226662531495\n",
      "bert.encoder.layer.5.attention.self.query.weight: 0.0053684646263718605\n",
      "bert.encoder.layer.5.attention.self.query.bias: 0.0002840582747012377\n",
      "bert.encoder.layer.5.attention.self.key.weight: 0.005310427397489548\n",
      "bert.encoder.layer.5.attention.self.key.bias: 7.992373429743793e-11\n",
      "bert.encoder.layer.5.attention.self.value.weight: 0.018426308408379555\n",
      "bert.encoder.layer.5.attention.self.value.bias: 0.002025920432060957\n",
      "bert.encoder.layer.5.attention.output.dense.weight: 0.01104049850255251\n",
      "bert.encoder.layer.5.attention.output.dense.bias: 0.0024430130142718554\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0014433915494009852\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.002274964237585664\n",
      "bert.encoder.layer.5.intermediate.dense.weight: 0.01880686730146408\n",
      "bert.encoder.layer.5.intermediate.dense.bias: 0.0007846274529583752\n",
      "bert.encoder.layer.5.output.dense.weight: 0.011014852672815323\n",
      "bert.encoder.layer.5.output.dense.bias: 0.0021937331184744835\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: 0.004588815849274397\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: 0.0033327534329146147\n",
      "bert.encoder.layer.6.attention.self.query.weight: 0.005818662233650684\n",
      "bert.encoder.layer.6.attention.self.query.bias: 0.00028509876574389637\n",
      "bert.encoder.layer.6.attention.self.key.weight: 0.005537192337214947\n",
      "bert.encoder.layer.6.attention.self.key.bias: 1.2951764660762421e-10\n",
      "bert.encoder.layer.6.attention.self.value.weight: 0.019221916794776917\n",
      "bert.encoder.layer.6.attention.self.value.bias: 0.0020062769763171673\n",
      "bert.encoder.layer.6.attention.output.dense.weight: 0.011485598981380463\n",
      "bert.encoder.layer.6.attention.output.dense.bias: 0.0025000786408782005\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.0013982249656692147\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.002392241731286049\n",
      "bert.encoder.layer.6.intermediate.dense.weight: 0.018814384937286377\n",
      "bert.encoder.layer.6.intermediate.dense.bias: 0.000707963714376092\n",
      "bert.encoder.layer.6.output.dense.weight: 0.01169535517692566\n",
      "bert.encoder.layer.6.output.dense.bias: 0.0023711626417934895\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: 0.0026557636447250843\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: 0.0036880681291222572\n",
      "bert.encoder.layer.7.attention.self.query.weight: 0.0038526535499840975\n",
      "bert.encoder.layer.7.attention.self.query.bias: 0.00019895490549970418\n",
      "bert.encoder.layer.7.attention.self.key.weight: 0.0038639414124190807\n",
      "bert.encoder.layer.7.attention.self.key.bias: 7.117918754406816e-11\n",
      "bert.encoder.layer.7.attention.self.value.weight: 0.019326332956552505\n",
      "bert.encoder.layer.7.attention.self.value.bias: 0.0019737891852855682\n",
      "bert.encoder.layer.7.attention.output.dense.weight: 0.010576303116977215\n",
      "bert.encoder.layer.7.attention.output.dense.bias: 0.0027812186162918806\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.0016246545128524303\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.0026227429043501616\n",
      "bert.encoder.layer.7.intermediate.dense.weight: 0.025495490059256554\n",
      "bert.encoder.layer.7.intermediate.dense.bias: 0.0008369887946173549\n",
      "bert.encoder.layer.7.output.dense.weight: 0.013843984343111515\n",
      "bert.encoder.layer.7.output.dense.bias: 0.002614187076687813\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: 0.00320997997187078\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: 0.004181906580924988\n",
      "bert.encoder.layer.8.attention.self.query.weight: 0.006273623555898666\n",
      "bert.encoder.layer.8.attention.self.query.bias: 0.0003502667532302439\n",
      "bert.encoder.layer.8.attention.self.key.weight: 0.005925006698817015\n",
      "bert.encoder.layer.8.attention.self.key.bias: 1.0695788005277151e-10\n",
      "bert.encoder.layer.8.attention.self.value.weight: 0.023429378867149353\n",
      "bert.encoder.layer.8.attention.self.value.bias: 0.0028236687649041414\n",
      "bert.encoder.layer.8.attention.output.dense.weight: 0.013588628731667995\n",
      "bert.encoder.layer.8.attention.output.dense.bias: 0.003372258972376585\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.0016939551569521427\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.0028791490476578474\n",
      "bert.encoder.layer.8.intermediate.dense.weight: 0.02545875869691372\n",
      "bert.encoder.layer.8.intermediate.dense.bias: 0.0008165011531673372\n",
      "bert.encoder.layer.8.output.dense.weight: 0.014348968863487244\n",
      "bert.encoder.layer.8.output.dense.bias: 0.0028704949654638767\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: 0.005036155227571726\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: 0.004266940988600254\n",
      "bert.encoder.layer.9.attention.self.query.weight: 0.007601430639624596\n",
      "bert.encoder.layer.9.attention.self.query.bias: 0.0004160151001997292\n",
      "bert.encoder.layer.9.attention.self.key.weight: 0.0064200423657894135\n",
      "bert.encoder.layer.9.attention.self.key.bias: 1.4149309501831908e-10\n",
      "bert.encoder.layer.9.attention.self.value.weight: 0.01970778964459896\n",
      "bert.encoder.layer.9.attention.self.value.bias: 0.0025363198947161436\n",
      "bert.encoder.layer.9.attention.output.dense.weight: 0.01607731729745865\n",
      "bert.encoder.layer.9.attention.output.dense.bias: 0.002911811228841543\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.0016955104656517506\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.002746734768152237\n",
      "bert.encoder.layer.9.intermediate.dense.weight: 0.019765274599194527\n",
      "bert.encoder.layer.9.intermediate.dense.bias: 0.0009065049234777689\n",
      "bert.encoder.layer.9.output.dense.weight: 0.015633180737495422\n",
      "bert.encoder.layer.9.output.dense.bias: 0.002862982451915741\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: 0.003521621460095048\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: 0.004317640792578459\n",
      "bert.encoder.layer.10.attention.self.query.weight: 0.007594997528940439\n",
      "bert.encoder.layer.10.attention.self.query.bias: 0.0003772901836782694\n",
      "bert.encoder.layer.10.attention.self.key.weight: 0.006695721298456192\n",
      "bert.encoder.layer.10.attention.self.key.bias: 1.962747048223079e-10\n",
      "bert.encoder.layer.10.attention.self.value.weight: 0.02302531898021698\n",
      "bert.encoder.layer.10.attention.self.value.bias: 0.0029375615995377302\n",
      "bert.encoder.layer.10.attention.output.dense.weight: 0.015263955108821392\n",
      "bert.encoder.layer.10.attention.output.dense.bias: 0.00269719073548913\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.0017770889680832624\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.00258735497482121\n",
      "bert.encoder.layer.10.intermediate.dense.weight: 0.024665826931595802\n",
      "bert.encoder.layer.10.intermediate.dense.bias: 0.0012077451683580875\n",
      "bert.encoder.layer.10.output.dense.weight: 0.014387701638042927\n",
      "bert.encoder.layer.10.output.dense.bias: 0.0024778475053608418\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: 0.002228622790426016\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: 0.003487546229735017\n",
      "bert.encoder.layer.11.attention.self.query.weight: 0.008472583256661892\n",
      "bert.encoder.layer.11.attention.self.query.bias: 0.0004943546373397112\n",
      "bert.encoder.layer.11.attention.self.key.weight: 0.008262068033218384\n",
      "bert.encoder.layer.11.attention.self.key.bias: 8.16972739459132e-11\n",
      "bert.encoder.layer.11.attention.self.value.weight: 0.027911754325032234\n",
      "bert.encoder.layer.11.attention.self.value.bias: 0.0022441053297370672\n",
      "bert.encoder.layer.11.attention.output.dense.weight: 0.013618323020637035\n",
      "bert.encoder.layer.11.attention.output.dense.bias: 0.0018362251576036215\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.0012218276970088482\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.0015696194022893906\n",
      "bert.encoder.layer.11.intermediate.dense.weight: 0.017761364579200745\n",
      "bert.encoder.layer.11.intermediate.dense.bias: 0.000518759828992188\n",
      "bert.encoder.layer.11.output.dense.weight: 0.015366656705737114\n",
      "bert.encoder.layer.11.output.dense.bias: 0.0015054624527692795\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: 0.004372526425868273\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: 0.003979123663157225\n",
      "bert.pooler.dense.weight: 0.12979041039943695\n",
      "bert.pooler.dense.bias: 0.01081771682947874\n",
      "fc.0.weight: 0.44044092297554016\n",
      "fc.0.bias: 0.026383329182863235\n",
      "fc.2.weight: 0.10515931993722916\n",
      "fc.2.bias: 0.058832548558712006\n",
      "Epoch: 1, Batch: 2, Loss: 0.7766414880752563\n",
      "bert.embeddings.word_embeddings.weight: 0.01000723522156477\n",
      "bert.embeddings.position_embeddings.weight: 0.009630734100937843\n",
      "bert.embeddings.token_type_embeddings.weight: 0.021021824330091476\n",
      "bert.embeddings.LayerNorm.weight: 0.0007021186174824834\n",
      "bert.embeddings.LayerNorm.bias: 0.0012211124412715435\n",
      "bert.encoder.layer.0.attention.self.query.weight: 0.0017499601235613227\n",
      "bert.encoder.layer.0.attention.self.query.bias: 0.00011576914403121918\n",
      "bert.encoder.layer.0.attention.self.key.weight: 0.001743457978591323\n",
      "bert.encoder.layer.0.attention.self.key.bias: 3.8190586110209424e-11\n",
      "bert.encoder.layer.0.attention.self.value.weight: 0.004401245154440403\n",
      "bert.encoder.layer.0.attention.self.value.bias: 0.000816904183011502\n",
      "bert.encoder.layer.0.attention.output.dense.weight: 0.004701783414930105\n",
      "bert.encoder.layer.0.attention.output.dense.bias: 0.0009662830270826817\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0004542920214589685\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0007146532880142331\n",
      "bert.encoder.layer.0.intermediate.dense.weight: 0.009670496918261051\n",
      "bert.encoder.layer.0.intermediate.dense.bias: 0.00029342935886234045\n",
      "bert.encoder.layer.0.output.dense.weight: 0.004915377590805292\n",
      "bert.encoder.layer.0.output.dense.bias: 0.0007006010855548084\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: 0.0007380744791589677\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: 0.0011816078331321478\n",
      "bert.encoder.layer.1.attention.self.query.weight: 0.002897467464208603\n",
      "bert.encoder.layer.1.attention.self.query.bias: 0.00015321429236792028\n",
      "bert.encoder.layer.1.attention.self.key.weight: 0.002978493692353368\n",
      "bert.encoder.layer.1.attention.self.key.bias: 5.884568526903777e-11\n",
      "bert.encoder.layer.1.attention.self.value.weight: 0.006664304528385401\n",
      "bert.encoder.layer.1.attention.self.value.bias: 0.0008126860484480858\n",
      "bert.encoder.layer.1.attention.output.dense.weight: 0.005441994871944189\n",
      "bert.encoder.layer.1.attention.output.dense.bias: 0.0009981640614569187\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0005583764286711812\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.0008181732264347374\n",
      "bert.encoder.layer.1.intermediate.dense.weight: 0.011988824233412743\n",
      "bert.encoder.layer.1.intermediate.dense.bias: 0.00033956620609387755\n",
      "bert.encoder.layer.1.output.dense.weight: 0.005398023407906294\n",
      "bert.encoder.layer.1.output.dense.bias: 0.0008122954750433564\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: 0.0016183954430744052\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: 0.001097208703868091\n",
      "bert.encoder.layer.2.attention.self.query.weight: 0.0029353785794228315\n",
      "bert.encoder.layer.2.attention.self.query.bias: 0.00018339295638725162\n",
      "bert.encoder.layer.2.attention.self.key.weight: 0.00257290992885828\n",
      "bert.encoder.layer.2.attention.self.key.bias: 6.010097974851192e-11\n",
      "bert.encoder.layer.2.attention.self.value.weight: 0.009404173120856285\n",
      "bert.encoder.layer.2.attention.self.value.bias: 0.0008506759768351912\n",
      "bert.encoder.layer.2.attention.output.dense.weight: 0.00647653779014945\n",
      "bert.encoder.layer.2.attention.output.dense.bias: 0.000974995840806514\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0006071845418773592\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0009153361315838993\n",
      "bert.encoder.layer.2.intermediate.dense.weight: 0.01227292325347662\n",
      "bert.encoder.layer.2.intermediate.dense.bias: 0.00047367080696858466\n",
      "bert.encoder.layer.2.output.dense.weight: 0.006277867592871189\n",
      "bert.encoder.layer.2.output.dense.bias: 0.0009833053918555379\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: 0.002553812228143215\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: 0.0014558264520019293\n",
      "bert.encoder.layer.3.attention.self.query.weight: 0.0036902441643178463\n",
      "bert.encoder.layer.3.attention.self.query.bias: 0.00030037277610972524\n",
      "bert.encoder.layer.3.attention.self.key.weight: 0.0031774360686540604\n",
      "bert.encoder.layer.3.attention.self.key.bias: 6.324979429095379e-11\n",
      "bert.encoder.layer.3.attention.self.value.weight: 0.010823218151926994\n",
      "bert.encoder.layer.3.attention.self.value.bias: 0.001150221098214388\n",
      "bert.encoder.layer.3.attention.output.dense.weight: 0.007229260168969631\n",
      "bert.encoder.layer.3.attention.output.dense.bias: 0.0012589390389621258\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0009630979038774967\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.0011278854217380285\n",
      "bert.encoder.layer.3.intermediate.dense.weight: 0.01315752137452364\n",
      "bert.encoder.layer.3.intermediate.dense.bias: 0.0004289171192795038\n",
      "bert.encoder.layer.3.output.dense.weight: 0.006302281282842159\n",
      "bert.encoder.layer.3.output.dense.bias: 0.0010664048604667187\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: 0.0018750341841951013\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: 0.0016552085289731622\n",
      "bert.encoder.layer.4.attention.self.query.weight: 0.0038688229396939278\n",
      "bert.encoder.layer.4.attention.self.query.bias: 0.00024012767244130373\n",
      "bert.encoder.layer.4.attention.self.key.weight: 0.003753532422706485\n",
      "bert.encoder.layer.4.attention.self.key.bias: 9.237763332059501e-11\n",
      "bert.encoder.layer.4.attention.self.value.weight: 0.01057051308453083\n",
      "bert.encoder.layer.4.attention.self.value.bias: 0.0011716457083821297\n",
      "bert.encoder.layer.4.attention.output.dense.weight: 0.006565704941749573\n",
      "bert.encoder.layer.4.attention.output.dense.bias: 0.0012363572604954243\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0009146216325461864\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.0011539815459400415\n",
      "bert.encoder.layer.4.intermediate.dense.weight: 0.012160860933363438\n",
      "bert.encoder.layer.4.intermediate.dense.bias: 0.00046965229557827115\n",
      "bert.encoder.layer.4.output.dense.weight: 0.0063613890670239925\n",
      "bert.encoder.layer.4.output.dense.bias: 0.0011839490616694093\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: 0.003020936856046319\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: 0.0018416376551613212\n",
      "bert.encoder.layer.5.attention.self.query.weight: 0.003185257548466325\n",
      "bert.encoder.layer.5.attention.self.query.bias: 0.000174616914591752\n",
      "bert.encoder.layer.5.attention.self.key.weight: 0.002913024742156267\n",
      "bert.encoder.layer.5.attention.self.key.bias: 4.7589727297792095e-11\n",
      "bert.encoder.layer.5.attention.self.value.weight: 0.010002787224948406\n",
      "bert.encoder.layer.5.attention.self.value.bias: 0.0010792112443596125\n",
      "bert.encoder.layer.5.attention.output.dense.weight: 0.006117814686149359\n",
      "bert.encoder.layer.5.attention.output.dense.bias: 0.0013578127836808562\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.001013789209537208\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.0012638126499950886\n",
      "bert.encoder.layer.5.intermediate.dense.weight: 0.010473912581801414\n",
      "bert.encoder.layer.5.intermediate.dense.bias: 0.0004016488092020154\n",
      "bert.encoder.layer.5.output.dense.weight: 0.006149891298264265\n",
      "bert.encoder.layer.5.output.dense.bias: 0.0012229896383360028\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: 0.0017947880551218987\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: 0.001844107056967914\n",
      "bert.encoder.layer.6.attention.self.query.weight: 0.0034321569837629795\n",
      "bert.encoder.layer.6.attention.self.query.bias: 0.00018841715063899755\n",
      "bert.encoder.layer.6.attention.self.key.weight: 0.003989525139331818\n",
      "bert.encoder.layer.6.attention.self.key.bias: 7.443812233276503e-11\n",
      "bert.encoder.layer.6.attention.self.value.weight: 0.011050909757614136\n",
      "bert.encoder.layer.6.attention.self.value.bias: 0.001136171631515026\n",
      "bert.encoder.layer.6.attention.output.dense.weight: 0.00657700328156352\n",
      "bert.encoder.layer.6.attention.output.dense.bias: 0.001353447325527668\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.0008608418283984065\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.0013213014462962747\n",
      "bert.encoder.layer.6.intermediate.dense.weight: 0.010599512606859207\n",
      "bert.encoder.layer.6.intermediate.dense.bias: 0.0004248924378771335\n",
      "bert.encoder.layer.6.output.dense.weight: 0.006728143896907568\n",
      "bert.encoder.layer.6.output.dense.bias: 0.001297383219935\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: 0.001252834452316165\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: 0.00199219211935997\n",
      "bert.encoder.layer.7.attention.self.query.weight: 0.002704297425225377\n",
      "bert.encoder.layer.7.attention.self.query.bias: 0.00013149880396667868\n",
      "bert.encoder.layer.7.attention.self.key.weight: 0.002525205723941326\n",
      "bert.encoder.layer.7.attention.self.key.bias: 6.358435999942458e-11\n",
      "bert.encoder.layer.7.attention.self.value.weight: 0.011473300866782665\n",
      "bert.encoder.layer.7.attention.self.value.bias: 0.0011354940943419933\n",
      "bert.encoder.layer.7.attention.output.dense.weight: 0.00608832947909832\n",
      "bert.encoder.layer.7.attention.output.dense.bias: 0.0015776578802615404\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.001127271680161357\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.001491022645495832\n",
      "bert.encoder.layer.7.intermediate.dense.weight: 0.013326565735042095\n",
      "bert.encoder.layer.7.intermediate.dense.bias: 0.00046030242810957134\n",
      "bert.encoder.layer.7.output.dense.weight: 0.007620563730597496\n",
      "bert.encoder.layer.7.output.dense.bias: 0.0015039826976135373\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: 0.0012458041310310364\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: 0.0023318903986364603\n",
      "bert.encoder.layer.8.attention.self.query.weight: 0.0037478867452591658\n",
      "bert.encoder.layer.8.attention.self.query.bias: 0.000257748004514724\n",
      "bert.encoder.layer.8.attention.self.key.weight: 0.0036962912417948246\n",
      "bert.encoder.layer.8.attention.self.key.bias: 6.304538835433249e-11\n",
      "bert.encoder.layer.8.attention.self.value.weight: 0.013946943916380405\n",
      "bert.encoder.layer.8.attention.self.value.bias: 0.0016624488634988666\n",
      "bert.encoder.layer.8.attention.output.dense.weight: 0.007345149293541908\n",
      "bert.encoder.layer.8.attention.output.dense.bias: 0.0019312829244881868\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.0019601022358983755\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.0016688266769051552\n",
      "bert.encoder.layer.8.intermediate.dense.weight: 0.01654709503054619\n",
      "bert.encoder.layer.8.intermediate.dense.bias: 0.0005475555080920458\n",
      "bert.encoder.layer.8.output.dense.weight: 0.008439474739134312\n",
      "bert.encoder.layer.8.output.dense.bias: 0.0017311356496065855\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: 0.0013188637094572186\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: 0.002703481586650014\n",
      "bert.encoder.layer.9.attention.self.query.weight: 0.004671778529882431\n",
      "bert.encoder.layer.9.attention.self.query.bias: 0.0002942140563391149\n",
      "bert.encoder.layer.9.attention.self.key.weight: 0.00385943497531116\n",
      "bert.encoder.layer.9.attention.self.key.bias: 8.360002273777312e-11\n",
      "bert.encoder.layer.9.attention.self.value.weight: 0.013017089106142521\n",
      "bert.encoder.layer.9.attention.self.value.bias: 0.0017107699532061815\n",
      "bert.encoder.layer.9.attention.output.dense.weight: 0.00945239420980215\n",
      "bert.encoder.layer.9.attention.output.dense.bias: 0.0017525959992781281\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.0019676084630191326\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.0016268491744995117\n",
      "bert.encoder.layer.9.intermediate.dense.weight: 0.01166521105915308\n",
      "bert.encoder.layer.9.intermediate.dense.bias: 0.0004830074612982571\n",
      "bert.encoder.layer.9.output.dense.weight: 0.008756163530051708\n",
      "bert.encoder.layer.9.output.dense.bias: 0.0016559666255488992\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: 0.002969869878143072\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: 0.0023271609097719193\n",
      "bert.encoder.layer.10.attention.self.query.weight: 0.004978525917977095\n",
      "bert.encoder.layer.10.attention.self.query.bias: 0.00027685356326401234\n",
      "bert.encoder.layer.10.attention.self.key.weight: 0.004304974805563688\n",
      "bert.encoder.layer.10.attention.self.key.bias: 8.984590849081542e-11\n",
      "bert.encoder.layer.10.attention.self.value.weight: 0.01237653847783804\n",
      "bert.encoder.layer.10.attention.self.value.bias: 0.0016637827502563596\n",
      "bert.encoder.layer.10.attention.output.dense.weight: 0.00810328684747219\n",
      "bert.encoder.layer.10.attention.output.dense.bias: 0.0014972637873142958\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.0019678263925015926\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.0014315021689981222\n",
      "bert.encoder.layer.10.intermediate.dense.weight: 0.014239533804357052\n",
      "bert.encoder.layer.10.intermediate.dense.bias: 0.0006826186436228454\n",
      "bert.encoder.layer.10.output.dense.weight: 0.007870033383369446\n",
      "bert.encoder.layer.10.output.dense.bias: 0.0013917236356064677\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: 0.0019062187056988478\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: 0.0018584545468911529\n",
      "bert.encoder.layer.11.attention.self.query.weight: 0.004111114423722029\n",
      "bert.encoder.layer.11.attention.self.query.bias: 0.00022307901235762984\n",
      "bert.encoder.layer.11.attention.self.key.weight: 0.0038194607477635145\n",
      "bert.encoder.layer.11.attention.self.key.bias: 1.0283469215055518e-10\n",
      "bert.encoder.layer.11.attention.self.value.weight: 0.01723555475473404\n",
      "bert.encoder.layer.11.attention.self.value.bias: 0.001319417729973793\n",
      "bert.encoder.layer.11.attention.output.dense.weight: 0.008205498568713665\n",
      "bert.encoder.layer.11.attention.output.dense.bias: 0.0010104677639901638\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.0009773449273779988\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.0008924307767301798\n",
      "bert.encoder.layer.11.intermediate.dense.weight: 0.011883188970386982\n",
      "bert.encoder.layer.11.intermediate.dense.bias: 0.0003412629012018442\n",
      "bert.encoder.layer.11.output.dense.weight: 0.008479069918394089\n",
      "bert.encoder.layer.11.output.dense.bias: 0.0008290964760817587\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: 0.0019832965917885303\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: 0.0016796652926132083\n",
      "bert.pooler.dense.weight: 0.062166038900613785\n",
      "bert.pooler.dense.bias: 0.004626461770385504\n",
      "fc.0.weight: 0.220940962433815\n",
      "fc.0.bias: 0.011776727624237537\n",
      "fc.2.weight: 0.053928643465042114\n",
      "fc.2.bias: 0.02636575512588024\n",
      "Epoch: 1, Batch: 3, Loss: 0.7169586420059204\n",
      "bert.embeddings.word_embeddings.weight: 0.010807155631482601\n",
      "bert.embeddings.position_embeddings.weight: 0.010570403188467026\n",
      "bert.embeddings.token_type_embeddings.weight: 0.024651795625686646\n",
      "bert.embeddings.LayerNorm.weight: 0.0009884195169433951\n",
      "bert.embeddings.LayerNorm.bias: 0.001446766429580748\n",
      "bert.encoder.layer.0.attention.self.query.weight: 0.0017270994139835238\n",
      "bert.encoder.layer.0.attention.self.query.bias: 9.772476187208667e-05\n",
      "bert.encoder.layer.0.attention.self.key.weight: 0.001653056824579835\n",
      "bert.encoder.layer.0.attention.self.key.bias: 4.8122696799657305e-11\n",
      "bert.encoder.layer.0.attention.self.value.weight: 0.004435643088072538\n",
      "bert.encoder.layer.0.attention.self.value.bias: 0.0008439567754976451\n",
      "bert.encoder.layer.0.attention.output.dense.weight: 0.0051300786435604095\n",
      "bert.encoder.layer.0.attention.output.dense.bias: 0.001050066901370883\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0004783073964063078\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0007740255095995963\n",
      "bert.encoder.layer.0.intermediate.dense.weight: 0.012290136888623238\n",
      "bert.encoder.layer.0.intermediate.dense.bias: 0.0003297356015536934\n",
      "bert.encoder.layer.0.output.dense.weight: 0.005544065032154322\n",
      "bert.encoder.layer.0.output.dense.bias: 0.0007954008760862052\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: 0.0022281575947999954\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: 0.0013863486237823963\n",
      "bert.encoder.layer.1.attention.self.query.weight: 0.0033366659190505743\n",
      "bert.encoder.layer.1.attention.self.query.bias: 0.00023619621060788631\n",
      "bert.encoder.layer.1.attention.self.key.weight: 0.003246777690947056\n",
      "bert.encoder.layer.1.attention.self.key.bias: 7.416473685184499e-11\n",
      "bert.encoder.layer.1.attention.self.value.weight: 0.00872961524873972\n",
      "bert.encoder.layer.1.attention.self.value.bias: 0.0011190291261300445\n",
      "bert.encoder.layer.1.attention.output.dense.weight: 0.006300483830273151\n",
      "bert.encoder.layer.1.attention.output.dense.bias: 0.0012414909433573484\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.0006107673398219049\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.001008415943942964\n",
      "bert.encoder.layer.1.intermediate.dense.weight: 0.01889309287071228\n",
      "bert.encoder.layer.1.intermediate.dense.bias: 0.0004917917540296912\n",
      "bert.encoder.layer.1.output.dense.weight: 0.0065017626620829105\n",
      "bert.encoder.layer.1.output.dense.bias: 0.0010878165485337377\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: 0.0020541129633784294\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: 0.0016235277289524674\n",
      "bert.encoder.layer.2.attention.self.query.weight: 0.002945183077827096\n",
      "bert.encoder.layer.2.attention.self.query.bias: 0.0002017278311541304\n",
      "bert.encoder.layer.2.attention.self.key.weight: 0.0029653452802449465\n",
      "bert.encoder.layer.2.attention.self.key.bias: 7.262532936147537e-11\n",
      "bert.encoder.layer.2.attention.self.value.weight: 0.015119137242436409\n",
      "bert.encoder.layer.2.attention.self.value.bias: 0.001603366108611226\n",
      "bert.encoder.layer.2.attention.output.dense.weight: 0.0082614840939641\n",
      "bert.encoder.layer.2.attention.output.dense.bias: 0.0014431759482249618\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0007688318728469312\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0013378497678786516\n",
      "bert.encoder.layer.2.intermediate.dense.weight: 0.01615777611732483\n",
      "bert.encoder.layer.2.intermediate.dense.bias: 0.0006342874839901924\n",
      "bert.encoder.layer.2.output.dense.weight: 0.0075180912390351295\n",
      "bert.encoder.layer.2.output.dense.bias: 0.0012791184708476067\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: 0.0013079942436888814\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: 0.0018372926861047745\n",
      "bert.encoder.layer.3.attention.self.query.weight: 0.004521271679550409\n",
      "bert.encoder.layer.3.attention.self.query.bias: 0.00025924009969457984\n",
      "bert.encoder.layer.3.attention.self.key.weight: 0.003820995334535837\n",
      "bert.encoder.layer.3.attention.self.key.bias: 7.021393882977733e-11\n",
      "bert.encoder.layer.3.attention.self.value.weight: 0.013637758791446686\n",
      "bert.encoder.layer.3.attention.self.value.bias: 0.0014083144487813115\n",
      "bert.encoder.layer.3.attention.output.dense.weight: 0.008861470967531204\n",
      "bert.encoder.layer.3.attention.output.dense.bias: 0.0015784254064783454\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0009055138798430562\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.0013896309537813067\n",
      "bert.encoder.layer.3.intermediate.dense.weight: 0.02024691179394722\n",
      "bert.encoder.layer.3.intermediate.dense.bias: 0.0006229655118659139\n",
      "bert.encoder.layer.3.output.dense.weight: 0.008011365309357643\n",
      "bert.encoder.layer.3.output.dense.bias: 0.0013599047670140862\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: 0.0020650988444685936\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: 0.0022335376124829054\n",
      "bert.encoder.layer.4.attention.self.query.weight: 0.005154342390596867\n",
      "bert.encoder.layer.4.attention.self.query.bias: 0.0003498316800687462\n",
      "bert.encoder.layer.4.attention.self.key.weight: 0.0053212689235806465\n",
      "bert.encoder.layer.4.attention.self.key.bias: 1.1283623341240556e-10\n",
      "bert.encoder.layer.4.attention.self.value.weight: 0.014856141991913319\n",
      "bert.encoder.layer.4.attention.self.value.bias: 0.0015928843058645725\n",
      "bert.encoder.layer.4.attention.output.dense.weight: 0.008251205086708069\n",
      "bert.encoder.layer.4.attention.output.dense.bias: 0.0015092712128534913\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0008645735215395689\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.001404828974045813\n",
      "bert.encoder.layer.4.intermediate.dense.weight: 0.017189661040902138\n",
      "bert.encoder.layer.4.intermediate.dense.bias: 0.0005869886372238398\n",
      "bert.encoder.layer.4.output.dense.weight: 0.007910490967333317\n",
      "bert.encoder.layer.4.output.dense.bias: 0.0013250638730823994\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: 0.003781714476644993\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: 0.002155244117602706\n",
      "bert.encoder.layer.5.attention.self.query.weight: 0.003869017818942666\n",
      "bert.encoder.layer.5.attention.self.query.bias: 0.00023066706489771605\n",
      "bert.encoder.layer.5.attention.self.key.weight: 0.003612891770899296\n",
      "bert.encoder.layer.5.attention.self.key.bias: 5.80888392942569e-11\n",
      "bert.encoder.layer.5.attention.self.value.weight: 0.012211241759359837\n",
      "bert.encoder.layer.5.attention.self.value.bias: 0.0012976564466953278\n",
      "bert.encoder.layer.5.attention.output.dense.weight: 0.007255919277667999\n",
      "bert.encoder.layer.5.attention.output.dense.bias: 0.0016241110861301422\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0009069794323295355\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.0014685590285807848\n",
      "bert.encoder.layer.5.intermediate.dense.weight: 0.014333452098071575\n",
      "bert.encoder.layer.5.intermediate.dense.bias: 0.0005249214009381831\n",
      "bert.encoder.layer.5.output.dense.weight: 0.007059536874294281\n",
      "bert.encoder.layer.5.output.dense.bias: 0.0013300186255946755\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: 0.002354757394641638\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: 0.0020805015228688717\n",
      "bert.encoder.layer.6.attention.self.query.weight: 0.0042213015258312225\n",
      "bert.encoder.layer.6.attention.self.query.bias: 0.00021098455181345344\n",
      "bert.encoder.layer.6.attention.self.key.weight: 0.004675519652664661\n",
      "bert.encoder.layer.6.attention.self.key.bias: 6.326744683704533e-11\n",
      "bert.encoder.layer.6.attention.self.value.weight: 0.012715977616608143\n",
      "bert.encoder.layer.6.attention.self.value.bias: 0.0012922221794724464\n",
      "bert.encoder.layer.6.attention.output.dense.weight: 0.0073324041441082954\n",
      "bert.encoder.layer.6.attention.output.dense.bias: 0.0014952797209843993\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.0010090934811159968\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.0014672267716377974\n",
      "bert.encoder.layer.6.intermediate.dense.weight: 0.01246875710785389\n",
      "bert.encoder.layer.6.intermediate.dense.bias: 0.0004758722789119929\n",
      "bert.encoder.layer.6.output.dense.weight: 0.007666162680834532\n",
      "bert.encoder.layer.6.output.dense.bias: 0.0014612384838983417\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: 0.0014253361150622368\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: 0.0022294404916465282\n",
      "bert.encoder.layer.7.attention.self.query.weight: 0.0028576485347002745\n",
      "bert.encoder.layer.7.attention.self.query.bias: 0.00016347972268704325\n",
      "bert.encoder.layer.7.attention.self.key.weight: 0.002870567375794053\n",
      "bert.encoder.layer.7.attention.self.key.bias: 5.766308264210096e-11\n",
      "bert.encoder.layer.7.attention.self.value.weight: 0.013144762255251408\n",
      "bert.encoder.layer.7.attention.self.value.bias: 0.0013069346314296126\n",
      "bert.encoder.layer.7.attention.output.dense.weight: 0.006964882370084524\n",
      "bert.encoder.layer.7.attention.output.dense.bias: 0.0017616943223401904\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.001138589926995337\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.0016642548143863678\n",
      "bert.encoder.layer.7.intermediate.dense.weight: 0.015632890164852142\n",
      "bert.encoder.layer.7.intermediate.dense.bias: 0.0005085629527457058\n",
      "bert.encoder.layer.7.output.dense.weight: 0.00868690200150013\n",
      "bert.encoder.layer.7.output.dense.bias: 0.001640019123442471\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: 0.0013533351011574268\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: 0.0025383231695741415\n",
      "bert.encoder.layer.8.attention.self.query.weight: 0.004456104710698128\n",
      "bert.encoder.layer.8.attention.self.query.bias: 0.000259167340118438\n",
      "bert.encoder.layer.8.attention.self.key.weight: 0.0039609381929039955\n",
      "bert.encoder.layer.8.attention.self.key.bias: 6.049529627238925e-11\n",
      "bert.encoder.layer.8.attention.self.value.weight: 0.015736130997538567\n",
      "bert.encoder.layer.8.attention.self.value.bias: 0.0018076080596074462\n",
      "bert.encoder.layer.8.attention.output.dense.weight: 0.00893359910696745\n",
      "bert.encoder.layer.8.attention.output.dense.bias: 0.0020313686691224575\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.0010859208414331079\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.0017575802048668265\n",
      "bert.encoder.layer.8.intermediate.dense.weight: 0.019991787150502205\n",
      "bert.encoder.layer.8.intermediate.dense.bias: 0.0006937661091797054\n",
      "bert.encoder.layer.8.output.dense.weight: 0.00946758221834898\n",
      "bert.encoder.layer.8.output.dense.bias: 0.0018405523151159286\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: 0.002869200427085161\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: 0.002840692177414894\n",
      "bert.encoder.layer.9.attention.self.query.weight: 0.0068796612322330475\n",
      "bert.encoder.layer.9.attention.self.query.bias: 0.0005009255837649107\n",
      "bert.encoder.layer.9.attention.self.key.weight: 0.005168430041521788\n",
      "bert.encoder.layer.9.attention.self.key.bias: 9.852980931146504e-11\n",
      "bert.encoder.layer.9.attention.self.value.weight: 0.01393298152834177\n",
      "bert.encoder.layer.9.attention.self.value.bias: 0.0018096101703122258\n",
      "bert.encoder.layer.9.attention.output.dense.weight: 0.009861372411251068\n",
      "bert.encoder.layer.9.attention.output.dense.bias: 0.0018139525782316923\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.0011634117690846324\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.0016759607242420316\n",
      "bert.encoder.layer.9.intermediate.dense.weight: 0.015053885988891125\n",
      "bert.encoder.layer.9.intermediate.dense.bias: 0.0007706323522143066\n",
      "bert.encoder.layer.9.output.dense.weight: 0.009881283156573772\n",
      "bert.encoder.layer.9.output.dense.bias: 0.0018059761496260762\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: 0.0049176947213709354\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: 0.0025422656908631325\n",
      "bert.encoder.layer.10.attention.self.query.weight: 0.006020287051796913\n",
      "bert.encoder.layer.10.attention.self.query.bias: 0.00036010576877743006\n",
      "bert.encoder.layer.10.attention.self.key.weight: 0.0054428912699222565\n",
      "bert.encoder.layer.10.attention.self.key.bias: 9.435605075047704e-11\n",
      "bert.encoder.layer.10.attention.self.value.weight: 0.015640100464224815\n",
      "bert.encoder.layer.10.attention.self.value.bias: 0.002304357010871172\n",
      "bert.encoder.layer.10.attention.output.dense.weight: 0.009484522044658661\n",
      "bert.encoder.layer.10.attention.output.dense.bias: 0.0017324259970337152\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.0012123348424211144\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.0016682237619534135\n",
      "bert.encoder.layer.10.intermediate.dense.weight: 0.018714940175414085\n",
      "bert.encoder.layer.10.intermediate.dense.bias: 0.0010274082887917757\n",
      "bert.encoder.layer.10.output.dense.weight: 0.008321116678416729\n",
      "bert.encoder.layer.10.output.dense.bias: 0.001442086067982018\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: 0.0011496448423713446\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: 0.0019905176013708115\n",
      "bert.encoder.layer.11.attention.self.query.weight: 0.005179884377866983\n",
      "bert.encoder.layer.11.attention.self.query.bias: 0.0002981582365464419\n",
      "bert.encoder.layer.11.attention.self.key.weight: 0.004765225574374199\n",
      "bert.encoder.layer.11.attention.self.key.bias: 8.45797668014292e-11\n",
      "bert.encoder.layer.11.attention.self.value.weight: 0.018481673672795296\n",
      "bert.encoder.layer.11.attention.self.value.bias: 0.0015143963973969221\n",
      "bert.encoder.layer.11.attention.output.dense.weight: 0.008358796127140522\n",
      "bert.encoder.layer.11.attention.output.dense.bias: 0.0009516780846752226\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.0007937060436233878\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.0008124917512759566\n",
      "bert.encoder.layer.11.intermediate.dense.weight: 0.011759291402995586\n",
      "bert.encoder.layer.11.intermediate.dense.bias: 0.0003696877392940223\n",
      "bert.encoder.layer.11.output.dense.weight: 0.007962381467223167\n",
      "bert.encoder.layer.11.output.dense.bias: 0.0006738304509781301\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: 0.0018369120080024004\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: 0.0015122031327337027\n",
      "bert.pooler.dense.weight: 0.04618283361196518\n",
      "bert.pooler.dense.bias: 0.003191571217030287\n",
      "fc.0.weight: 0.17668265104293823\n",
      "fc.0.bias: 0.009173262864351273\n",
      "fc.2.weight: 0.045038577169179916\n",
      "fc.2.bias: 0.020746592432260513\n",
      "Epoch: 1, Batch: 4, Loss: 0.6375460624694824\n",
      "bert.embeddings.word_embeddings.weight: 0.016456669196486473\n",
      "bert.embeddings.position_embeddings.weight: 0.014390205033123493\n",
      "bert.embeddings.token_type_embeddings.weight: 0.046609170734882355\n",
      "bert.embeddings.LayerNorm.weight: 0.0016427827067673206\n",
      "bert.embeddings.LayerNorm.bias: 0.0027940168511122465\n",
      "bert.encoder.layer.0.attention.self.query.weight: 0.003017471404746175\n",
      "bert.encoder.layer.0.attention.self.query.bias: 0.00021268209093250334\n",
      "bert.encoder.layer.0.attention.self.key.weight: 0.002789389342069626\n",
      "bert.encoder.layer.0.attention.self.key.bias: 5.995104412903629e-11\n",
      "bert.encoder.layer.0.attention.self.value.weight: 0.007987262681126595\n",
      "bert.encoder.layer.0.attention.self.value.bias: 0.0016360749723389745\n",
      "bert.encoder.layer.0.attention.output.dense.weight: 0.0097943264991045\n",
      "bert.encoder.layer.0.attention.output.dense.bias: 0.002066784305498004\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight: 0.0008618401479907334\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias: 0.0015190525446087122\n",
      "bert.encoder.layer.0.intermediate.dense.weight: 0.024432692676782608\n",
      "bert.encoder.layer.0.intermediate.dense.bias: 0.0006626895046792924\n",
      "bert.encoder.layer.0.output.dense.weight: 0.01046719029545784\n",
      "bert.encoder.layer.0.output.dense.bias: 0.0015543735353276134\n",
      "bert.encoder.layer.0.output.LayerNorm.weight: 0.005089350510388613\n",
      "bert.encoder.layer.0.output.LayerNorm.bias: 0.0029261531308293343\n",
      "bert.encoder.layer.1.attention.self.query.weight: 0.0052249012514948845\n",
      "bert.encoder.layer.1.attention.self.query.bias: 0.0003381710557732731\n",
      "bert.encoder.layer.1.attention.self.key.weight: 0.0054974304512143135\n",
      "bert.encoder.layer.1.attention.self.key.bias: 1.2733783472107518e-10\n",
      "bert.encoder.layer.1.attention.self.value.weight: 0.015314536169171333\n",
      "bert.encoder.layer.1.attention.self.value.bias: 0.0020545809529721737\n",
      "bert.encoder.layer.1.attention.output.dense.weight: 0.011894273571670055\n",
      "bert.encoder.layer.1.attention.output.dense.bias: 0.002495924709364772\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight: 0.001273937989026308\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias: 0.001966827781870961\n",
      "bert.encoder.layer.1.intermediate.dense.weight: 0.03252892941236496\n",
      "bert.encoder.layer.1.intermediate.dense.bias: 0.0008034849888645113\n",
      "bert.encoder.layer.1.output.dense.weight: 0.011724952608346939\n",
      "bert.encoder.layer.1.output.dense.bias: 0.0020343190990388393\n",
      "bert.encoder.layer.1.output.LayerNorm.weight: 0.002254365710541606\n",
      "bert.encoder.layer.1.output.LayerNorm.bias: 0.0031572829466313124\n",
      "bert.encoder.layer.2.attention.self.query.weight: 0.005496058613061905\n",
      "bert.encoder.layer.2.attention.self.query.bias: 0.0004094697942491621\n",
      "bert.encoder.layer.2.attention.self.key.weight: 0.0050150565803050995\n",
      "bert.encoder.layer.2.attention.self.key.bias: 1.1410635630815236e-10\n",
      "bert.encoder.layer.2.attention.self.value.weight: 0.02681960165500641\n",
      "bert.encoder.layer.2.attention.self.value.bias: 0.0028857996221631765\n",
      "bert.encoder.layer.2.attention.output.dense.weight: 0.01431650947779417\n",
      "bert.encoder.layer.2.attention.output.dense.bias: 0.0026400715578347445\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight: 0.0012969938106834888\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias: 0.0024649195838719606\n",
      "bert.encoder.layer.2.intermediate.dense.weight: 0.031041814014315605\n",
      "bert.encoder.layer.2.intermediate.dense.bias: 0.0013021937338635325\n",
      "bert.encoder.layer.2.output.dense.weight: 0.012615259736776352\n",
      "bert.encoder.layer.2.output.dense.bias: 0.0024641428608447313\n",
      "bert.encoder.layer.2.output.LayerNorm.weight: 0.0021205591037869453\n",
      "bert.encoder.layer.2.output.LayerNorm.bias: 0.0037078482564538717\n",
      "bert.encoder.layer.3.attention.self.query.weight: 0.008215932175517082\n",
      "bert.encoder.layer.3.attention.self.query.bias: 0.0005804997053928673\n",
      "bert.encoder.layer.3.attention.self.key.weight: 0.006607614923268557\n",
      "bert.encoder.layer.3.attention.self.key.bias: 9.339086448623135e-11\n",
      "bert.encoder.layer.3.attention.self.value.weight: 0.025631161406636238\n",
      "bert.encoder.layer.3.attention.self.value.bias: 0.0029246690683066845\n",
      "bert.encoder.layer.3.attention.output.dense.weight: 0.014440729282796383\n",
      "bert.encoder.layer.3.attention.output.dense.bias: 0.003083494957536459\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight: 0.0014625057810917497\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias: 0.00269414484500885\n",
      "bert.encoder.layer.3.intermediate.dense.weight: 0.03963546082377434\n",
      "bert.encoder.layer.3.intermediate.dense.bias: 0.001190120354294777\n",
      "bert.encoder.layer.3.output.dense.weight: 0.013471676036715508\n",
      "bert.encoder.layer.3.output.dense.bias: 0.002664465457201004\n",
      "bert.encoder.layer.3.output.LayerNorm.weight: 0.004401030018925667\n",
      "bert.encoder.layer.3.output.LayerNorm.bias: 0.0044379583559930325\n",
      "bert.encoder.layer.4.attention.self.query.weight: 0.008826542645692825\n",
      "bert.encoder.layer.4.attention.self.query.bias: 0.0005555952084250748\n",
      "bert.encoder.layer.4.attention.self.key.weight: 0.0078500434756279\n",
      "bert.encoder.layer.4.attention.self.key.bias: 1.4504419887373388e-10\n",
      "bert.encoder.layer.4.attention.self.value.weight: 0.026036038994789124\n",
      "bert.encoder.layer.4.attention.self.value.bias: 0.0030910761561244726\n",
      "bert.encoder.layer.4.attention.output.dense.weight: 0.01312909834086895\n",
      "bert.encoder.layer.4.attention.output.dense.bias: 0.0028720623813569546\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight: 0.0015666099498048425\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias: 0.0026137414388358593\n",
      "bert.encoder.layer.4.intermediate.dense.weight: 0.02729172259569168\n",
      "bert.encoder.layer.4.intermediate.dense.bias: 0.000925246044062078\n",
      "bert.encoder.layer.4.output.dense.weight: 0.012423989363014698\n",
      "bert.encoder.layer.4.output.dense.bias: 0.002403223654255271\n",
      "bert.encoder.layer.4.output.LayerNorm.weight: 0.002925567328929901\n",
      "bert.encoder.layer.4.output.LayerNorm.bias: 0.004084238316863775\n",
      "bert.encoder.layer.5.attention.self.query.weight: 0.004778159316629171\n",
      "bert.encoder.layer.5.attention.self.query.bias: 0.00028077230672352016\n",
      "bert.encoder.layer.5.attention.self.key.weight: 0.004458210896700621\n",
      "bert.encoder.layer.5.attention.self.key.bias: 8.125315698048752e-11\n",
      "bert.encoder.layer.5.attention.self.value.weight: 0.021734517067670822\n",
      "bert.encoder.layer.5.attention.self.value.bias: 0.0025341727305203676\n",
      "bert.encoder.layer.5.attention.output.dense.weight: 0.010938598774373531\n",
      "bert.encoder.layer.5.attention.output.dense.bias: 0.0029109970200806856\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight: 0.0017494900384917855\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias: 0.002622021362185478\n",
      "bert.encoder.layer.5.intermediate.dense.weight: 0.022617511451244354\n",
      "bert.encoder.layer.5.intermediate.dense.bias: 0.0008780934731476009\n",
      "bert.encoder.layer.5.output.dense.weight: 0.011145810596644878\n",
      "bert.encoder.layer.5.output.dense.bias: 0.002358803292736411\n",
      "bert.encoder.layer.5.output.LayerNorm.weight: 0.006783886346966028\n",
      "bert.encoder.layer.5.output.LayerNorm.bias: 0.0038583159912377596\n",
      "bert.encoder.layer.6.attention.self.query.weight: 0.006660806480795145\n",
      "bert.encoder.layer.6.attention.self.query.bias: 0.00048171685193665326\n",
      "bert.encoder.layer.6.attention.self.key.weight: 0.007206350099295378\n",
      "bert.encoder.layer.6.attention.self.key.bias: 9.149100921312936e-11\n",
      "bert.encoder.layer.6.attention.self.value.weight: 0.021670198068022728\n",
      "bert.encoder.layer.6.attention.self.value.bias: 0.0023220202419906855\n",
      "bert.encoder.layer.6.attention.output.dense.weight: 0.011138140223920345\n",
      "bert.encoder.layer.6.attention.output.dense.bias: 0.002694826340302825\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight: 0.0017803264781832695\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias: 0.002559864893555641\n",
      "bert.encoder.layer.6.intermediate.dense.weight: 0.021439172327518463\n",
      "bert.encoder.layer.6.intermediate.dense.bias: 0.0008091002819128335\n",
      "bert.encoder.layer.6.output.dense.weight: 0.012082884088158607\n",
      "bert.encoder.layer.6.output.dense.bias: 0.0025157323107123375\n",
      "bert.encoder.layer.6.output.LayerNorm.weight: 0.005322082433849573\n",
      "bert.encoder.layer.6.output.LayerNorm.bias: 0.003940823022276163\n",
      "bert.encoder.layer.7.attention.self.query.weight: 0.003952161408960819\n",
      "bert.encoder.layer.7.attention.self.query.bias: 0.00019950728164985776\n",
      "bert.encoder.layer.7.attention.self.key.weight: 0.0034724704455584288\n",
      "bert.encoder.layer.7.attention.self.key.bias: 5.199352406948243e-11\n",
      "bert.encoder.layer.7.attention.self.value.weight: 0.02142721228301525\n",
      "bert.encoder.layer.7.attention.self.value.bias: 0.00224744388833642\n",
      "bert.encoder.layer.7.attention.output.dense.weight: 0.010118099860846996\n",
      "bert.encoder.layer.7.attention.output.dense.bias: 0.00294599705375731\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight: 0.0021158584859222174\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias: 0.0027792025357484818\n",
      "bert.encoder.layer.7.intermediate.dense.weight: 0.02467661164700985\n",
      "bert.encoder.layer.7.intermediate.dense.bias: 0.0008502477430738509\n",
      "bert.encoder.layer.7.output.dense.weight: 0.013198231346905231\n",
      "bert.encoder.layer.7.output.dense.bias: 0.0027026839088648558\n",
      "bert.encoder.layer.7.output.LayerNorm.weight: 0.002665571868419647\n",
      "bert.encoder.layer.7.output.LayerNorm.bias: 0.0043176100589334965\n",
      "bert.encoder.layer.8.attention.self.query.weight: 0.006602860987186432\n",
      "bert.encoder.layer.8.attention.self.query.bias: 0.000403400365030393\n",
      "bert.encoder.layer.8.attention.self.key.weight: 0.006213000044226646\n",
      "bert.encoder.layer.8.attention.self.key.bias: 1.0422079171901188e-10\n",
      "bert.encoder.layer.8.attention.self.value.weight: 0.024618862196803093\n",
      "bert.encoder.layer.8.attention.self.value.bias: 0.002914684358984232\n",
      "bert.encoder.layer.8.attention.output.dense.weight: 0.013566344976425171\n",
      "bert.encoder.layer.8.attention.output.dense.bias: 0.0033396040089428425\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight: 0.0020113287027925253\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias: 0.0028895193245261908\n",
      "bert.encoder.layer.8.intermediate.dense.weight: 0.030814090743660927\n",
      "bert.encoder.layer.8.intermediate.dense.bias: 0.0010405754437670112\n",
      "bert.encoder.layer.8.output.dense.weight: 0.014479333534836769\n",
      "bert.encoder.layer.8.output.dense.bias: 0.003014578018337488\n",
      "bert.encoder.layer.8.output.LayerNorm.weight: 0.00663367472589016\n",
      "bert.encoder.layer.8.output.LayerNorm.bias: 0.004687110893428326\n",
      "bert.encoder.layer.9.attention.self.query.weight: 0.009353762492537498\n",
      "bert.encoder.layer.9.attention.self.query.bias: 0.0005370691069401801\n",
      "bert.encoder.layer.9.attention.self.key.weight: 0.007404658477753401\n",
      "bert.encoder.layer.9.attention.self.key.bias: 1.288852219394343e-10\n",
      "bert.encoder.layer.9.attention.self.value.weight: 0.02261943556368351\n",
      "bert.encoder.layer.9.attention.self.value.bias: 0.0029574583750218153\n",
      "bert.encoder.layer.9.attention.output.dense.weight: 0.014830083586275578\n",
      "bert.encoder.layer.9.attention.output.dense.bias: 0.00287783145904541\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight: 0.0019798020366579294\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias: 0.00261695240624249\n",
      "bert.encoder.layer.9.intermediate.dense.weight: 0.019099805504083633\n",
      "bert.encoder.layer.9.intermediate.dense.bias: 0.0008826330886222422\n",
      "bert.encoder.layer.9.output.dense.weight: 0.01410439983010292\n",
      "bert.encoder.layer.9.output.dense.bias: 0.0026622444856911898\n",
      "bert.encoder.layer.9.output.LayerNorm.weight: 0.007594819646328688\n",
      "bert.encoder.layer.9.output.LayerNorm.bias: 0.0038860617205500603\n",
      "bert.encoder.layer.10.attention.self.query.weight: 0.008747559040784836\n",
      "bert.encoder.layer.10.attention.self.query.bias: 0.0004922666121274233\n",
      "bert.encoder.layer.10.attention.self.key.weight: 0.007456168532371521\n",
      "bert.encoder.layer.10.attention.self.key.bias: 1.9128612582797189e-10\n",
      "bert.encoder.layer.10.attention.self.value.weight: 0.02026207186281681\n",
      "bert.encoder.layer.10.attention.self.value.bias: 0.0028823725879192352\n",
      "bert.encoder.layer.10.attention.output.dense.weight: 0.011764573864638805\n",
      "bert.encoder.layer.10.attention.output.dense.bias: 0.002427641535177827\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight: 0.0026569573674350977\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias: 0.0023293911945074797\n",
      "bert.encoder.layer.10.intermediate.dense.weight: 0.02779368683695793\n",
      "bert.encoder.layer.10.intermediate.dense.bias: 0.0013963787350803614\n",
      "bert.encoder.layer.10.output.dense.weight: 0.011723591014742851\n",
      "bert.encoder.layer.10.output.dense.bias: 0.0022005161736160517\n",
      "bert.encoder.layer.10.output.LayerNorm.weight: 0.0023506516590714455\n",
      "bert.encoder.layer.10.output.LayerNorm.bias: 0.002874691504985094\n",
      "bert.encoder.layer.11.attention.self.query.weight: 0.006784957833588123\n",
      "bert.encoder.layer.11.attention.self.query.bias: 0.0004013091966044158\n",
      "bert.encoder.layer.11.attention.self.key.weight: 0.005999837536364794\n",
      "bert.encoder.layer.11.attention.self.key.bias: 1.4792220226489405e-10\n",
      "bert.encoder.layer.11.attention.self.value.weight: 0.02686423808336258\n",
      "bert.encoder.layer.11.attention.self.value.bias: 0.0020655395928770304\n",
      "bert.encoder.layer.11.attention.output.dense.weight: 0.011818904429674149\n",
      "bert.encoder.layer.11.attention.output.dense.bias: 0.0012756185606122017\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight: 0.0011421043891459703\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias: 0.001166623318567872\n",
      "bert.encoder.layer.11.intermediate.dense.weight: 0.016575217247009277\n",
      "bert.encoder.layer.11.intermediate.dense.bias: 0.0005775234312750399\n",
      "bert.encoder.layer.11.output.dense.weight: 0.012095892801880836\n",
      "bert.encoder.layer.11.output.dense.bias: 0.0008357648039236665\n",
      "bert.encoder.layer.11.output.LayerNorm.weight: 0.002315258840098977\n",
      "bert.encoder.layer.11.output.LayerNorm.bias: 0.0017124046571552753\n",
      "bert.pooler.dense.weight: 0.05040259659290314\n",
      "bert.pooler.dense.bias: 0.0031848347280174494\n",
      "fc.0.weight: 0.23614764213562012\n",
      "fc.0.bias: 0.010993178933858871\n",
      "fc.2.weight: 0.0613577738404274\n",
      "fc.2.bias: 0.025613855570554733\n",
      "Epoch: 1, Batch: 5, Loss: 0.7150441408157349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(news)\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m     48\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, labels)\n\u001B[1;32m---> 49\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m param\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
