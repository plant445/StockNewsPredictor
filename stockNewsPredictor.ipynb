{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T04:57:01.692760Z",
     "start_time": "2024-11-24T04:57:01.673071Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T04:57:22.233453Z",
     "start_time": "2024-11-24T04:57:07.450677Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load datasets\n",
    "news_data = pd.read_csv(\"news.csv\")\n",
    "stock_data = pd.read_csv(\"table.csv\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Combine Top1-Top25 into a single string\n",
    "news_data['combined_text'] = news_data.iloc[:, 2:].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "\n",
    "# Tokenize headlines\n",
    "news_data['tokens'] = news_data['combined_text'].apply(\n",
    "    lambda x: tokenizer(x, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    ")\n",
    "\n",
    "# Scale stock data\n",
    "scaler = MinMaxScaler()\n",
    "stock_data[['Open', 'Close', 'High', 'Low', 'Volume']] = scaler.fit_transform(\n",
    "    stock_data[['Open', 'Close', 'High', 'Low', 'Volume']]\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enoch\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T04:57:57.219235Z",
     "start_time": "2024-11-24T04:57:57.216158Z"
    }
   },
   "source": [
    "class StockPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.stock_fc = nn.Sequential(\n",
    "            nn.Linear(5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768 + 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, stock_features):\n",
    "        # Text embeddings\n",
    "        bert_output = self.bert(**tokens).pooler_output\n",
    "        # Stock features\n",
    "        stock_output = self.stock_fc(stock_features)\n",
    "        # Concatenate and predict\n",
    "        combined = torch.cat((bert_output, stock_output), dim=1)\n",
    "        return torch.sigmoid(self.fc(combined))\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T07:18:11.356530Z",
     "start_time": "2024-11-24T05:15:05.810846Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, AdamW\n",
    "import torch\n",
    "torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Define Dataset Class\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, news, stocks, labels):\n",
    "        self.news = news\n",
    "        self.stocks = torch.tensor(stocks, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.news[idx], self.stocks[idx], self.labels[idx]\n",
    "\n",
    "# Prepare tokenized text data\n",
    "train_news = news_data['tokens'].tolist()  # This contains a list of dictionaries\n",
    "train_stocks = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "train_labels = news_data['Label'].values\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = StockDataset(train_news, train_stocks, train_labels)\n",
    "\n",
    "# Define Collate Function\n",
    "def collate_fn(batch):\n",
    "    news = {key: torch.cat([b[0][key] for b in batch], dim=0) for key in batch[0][0].keys()}  # Tokenized text\n",
    "    stocks = torch.stack([b[1] for b in batch])  # Stock features\n",
    "    labels = torch.stack([b[2] for b in batch])  # Labels\n",
    "    return news, stocks, labels\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define Model\n",
    "model = StockPredictor()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    batch_index = 0\n",
    "    for batch in train_loader:\n",
    "        news, stocks, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(news, stocks).squeeze()\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_index += 1\n",
    "        print(f\"Epoch: {epoch+1}, Batch: {batch_index}, Loss: {loss.item()}\")\n",
    "            "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch: 1, Batch: 1, Loss: 0.6669305562973022\n",
      "Epoch: 1, Batch: 2, Loss: 0.7550961971282959\n",
      "Epoch: 1, Batch: 3, Loss: 0.6785579919815063\n",
      "Epoch: 1, Batch: 4, Loss: 0.6741546988487244\n",
      "Epoch: 1, Batch: 5, Loss: 0.7743962407112122\n",
      "Epoch: 1, Batch: 6, Loss: 0.7721146941184998\n",
      "Epoch: 1, Batch: 7, Loss: 0.6914326548576355\n",
      "Epoch: 1, Batch: 8, Loss: 0.6880688667297363\n",
      "Epoch: 1, Batch: 9, Loss: 0.7214998006820679\n",
      "Epoch: 1, Batch: 10, Loss: 0.6713804006576538\n",
      "Epoch: 1, Batch: 11, Loss: 0.6932873129844666\n",
      "Epoch: 1, Batch: 12, Loss: 0.6563521027565002\n",
      "Epoch: 1, Batch: 13, Loss: 0.6166840195655823\n",
      "Epoch: 1, Batch: 14, Loss: 0.6846472024917603\n",
      "Epoch: 1, Batch: 15, Loss: 0.8005673885345459\n",
      "Epoch: 1, Batch: 16, Loss: 0.7182035446166992\n",
      "Epoch: 1, Batch: 17, Loss: 0.7665877938270569\n",
      "Epoch: 1, Batch: 18, Loss: 0.7620524168014526\n",
      "Epoch: 1, Batch: 19, Loss: 0.7028602361679077\n",
      "Epoch: 1, Batch: 20, Loss: 0.7382911443710327\n",
      "Epoch: 1, Batch: 21, Loss: 0.6687521934509277\n",
      "Epoch: 1, Batch: 22, Loss: 0.6693801879882812\n",
      "Epoch: 1, Batch: 23, Loss: 0.855156660079956\n",
      "Epoch: 1, Batch: 24, Loss: 0.6954288482666016\n",
      "Epoch: 1, Batch: 25, Loss: 0.6880939602851868\n",
      "Epoch: 1, Batch: 26, Loss: 0.6679306030273438\n",
      "Epoch: 1, Batch: 27, Loss: 0.7200422286987305\n",
      "Epoch: 1, Batch: 28, Loss: 0.7007002830505371\n",
      "Epoch: 1, Batch: 29, Loss: 0.6841920614242554\n",
      "Epoch: 1, Batch: 30, Loss: 0.7074122428894043\n",
      "Epoch: 1, Batch: 31, Loss: 0.8005719780921936\n",
      "Epoch: 1, Batch: 32, Loss: 0.754336416721344\n",
      "Epoch: 1, Batch: 33, Loss: 0.7129725217819214\n",
      "Epoch: 1, Batch: 34, Loss: 0.6834157705307007\n",
      "Epoch: 1, Batch: 35, Loss: 0.697441041469574\n",
      "Epoch: 1, Batch: 36, Loss: 0.6988982558250427\n",
      "Epoch: 1, Batch: 37, Loss: 0.6987162232398987\n",
      "Epoch: 1, Batch: 38, Loss: 0.6709527969360352\n",
      "Epoch: 1, Batch: 39, Loss: 0.7093740701675415\n",
      "Epoch: 1, Batch: 40, Loss: 0.6684534549713135\n",
      "Epoch: 1, Batch: 41, Loss: 0.7287272214889526\n",
      "Epoch: 1, Batch: 42, Loss: 0.7501387000083923\n",
      "Epoch: 1, Batch: 43, Loss: 0.705672562122345\n",
      "Epoch: 1, Batch: 44, Loss: 0.6713188290596008\n",
      "Epoch: 1, Batch: 45, Loss: 0.7100930213928223\n",
      "Epoch: 1, Batch: 46, Loss: 0.689740777015686\n",
      "Epoch: 1, Batch: 47, Loss: 0.6696130037307739\n",
      "Epoch: 1, Batch: 48, Loss: 0.717892050743103\n",
      "Epoch: 1, Batch: 49, Loss: 0.6622107028961182\n",
      "Epoch: 1, Batch: 50, Loss: 0.6738837957382202\n",
      "Epoch: 1, Batch: 51, Loss: 0.683490514755249\n",
      "Epoch: 1, Batch: 52, Loss: 0.7544001340866089\n",
      "Epoch: 1, Batch: 53, Loss: 0.7578518986701965\n",
      "Epoch: 1, Batch: 54, Loss: 0.6871782541275024\n",
      "Epoch: 1, Batch: 55, Loss: 0.6709219813346863\n",
      "Epoch: 1, Batch: 56, Loss: 0.715428352355957\n",
      "Epoch: 1, Batch: 57, Loss: 0.633029580116272\n",
      "Epoch: 1, Batch: 58, Loss: 0.563144326210022\n",
      "Epoch: 1, Batch: 59, Loss: 0.7081326246261597\n",
      "Epoch: 1, Batch: 60, Loss: 0.7592199444770813\n",
      "Epoch: 1, Batch: 61, Loss: 0.7911582589149475\n",
      "Epoch: 1, Batch: 62, Loss: 0.7244651317596436\n",
      "Epoch: 1, Batch: 63, Loss: 0.6964005827903748\n",
      "Epoch: 1, Batch: 64, Loss: 0.7167016267776489\n",
      "Epoch: 1, Batch: 65, Loss: 0.7083249092102051\n",
      "Epoch: 1, Batch: 66, Loss: 0.6925151944160461\n",
      "Epoch: 1, Batch: 67, Loss: 0.6869819164276123\n",
      "Epoch: 1, Batch: 68, Loss: 0.7022057771682739\n",
      "Epoch: 1, Batch: 69, Loss: 0.7517575025558472\n",
      "Epoch: 1, Batch: 70, Loss: 0.6857230067253113\n",
      "Epoch: 1, Batch: 71, Loss: 0.6429607272148132\n",
      "Epoch: 1, Batch: 72, Loss: 0.7080866098403931\n",
      "Epoch: 1, Batch: 73, Loss: 0.6888155937194824\n",
      "Epoch: 1, Batch: 74, Loss: 0.7305586338043213\n",
      "Epoch: 1, Batch: 75, Loss: 0.780003547668457\n",
      "Epoch: 1, Batch: 76, Loss: 0.7059658169746399\n",
      "Epoch: 1, Batch: 77, Loss: 0.7001247406005859\n",
      "Epoch: 1, Batch: 78, Loss: 0.6861761808395386\n",
      "Epoch: 1, Batch: 79, Loss: 0.6974729895591736\n",
      "Epoch: 1, Batch: 80, Loss: 0.739682137966156\n",
      "Epoch: 1, Batch: 81, Loss: 0.7090728282928467\n",
      "Epoch: 1, Batch: 82, Loss: 0.6145198345184326\n",
      "Epoch: 1, Batch: 83, Loss: 0.7181865572929382\n",
      "Epoch: 1, Batch: 84, Loss: 0.7182168364524841\n",
      "Epoch: 1, Batch: 85, Loss: 0.6665748357772827\n",
      "Epoch: 1, Batch: 86, Loss: 0.6340296268463135\n",
      "Epoch: 1, Batch: 87, Loss: 0.6299496293067932\n",
      "Epoch: 1, Batch: 88, Loss: 0.6323473453521729\n",
      "Epoch: 1, Batch: 89, Loss: 0.7258976697921753\n",
      "Epoch: 1, Batch: 90, Loss: 0.6657783389091492\n",
      "Epoch: 1, Batch: 91, Loss: 0.731086015701294\n",
      "Epoch: 1, Batch: 92, Loss: 0.6267674565315247\n",
      "Epoch: 1, Batch: 93, Loss: 0.6286801099777222\n",
      "Epoch: 1, Batch: 94, Loss: 0.6288484334945679\n",
      "Epoch: 1, Batch: 95, Loss: 0.5922252535820007\n",
      "Epoch: 1, Batch: 96, Loss: 0.7793550491333008\n",
      "Epoch: 1, Batch: 97, Loss: 0.5859419107437134\n",
      "Epoch: 1, Batch: 98, Loss: 0.7483352422714233\n",
      "Epoch: 1, Batch: 99, Loss: 0.7358725666999817\n",
      "Epoch: 1, Batch: 100, Loss: 0.7326124906539917\n",
      "Epoch: 1, Batch: 101, Loss: 0.721228837966919\n",
      "Epoch: 1, Batch: 102, Loss: 0.7047895789146423\n",
      "Epoch: 1, Batch: 103, Loss: 0.6700997352600098\n",
      "Epoch: 1, Batch: 104, Loss: 0.7097072005271912\n",
      "Epoch: 1, Batch: 105, Loss: 0.6703282594680786\n",
      "Epoch: 1, Batch: 106, Loss: 0.6822355389595032\n",
      "Epoch: 1, Batch: 107, Loss: 0.6941616535186768\n",
      "Epoch: 1, Batch: 108, Loss: 0.6852000951766968\n",
      "Epoch: 1, Batch: 109, Loss: 0.7243226766586304\n",
      "Epoch: 1, Batch: 110, Loss: 0.7160489559173584\n",
      "Epoch: 1, Batch: 111, Loss: 0.6909878849983215\n",
      "Epoch: 1, Batch: 112, Loss: 0.6935509443283081\n",
      "Epoch: 1, Batch: 113, Loss: 0.6954275369644165\n",
      "Epoch: 1, Batch: 114, Loss: 0.6946815848350525\n",
      "Epoch: 1, Batch: 115, Loss: 0.6898651123046875\n",
      "Epoch: 1, Batch: 116, Loss: 0.6882796883583069\n",
      "Epoch: 1, Batch: 117, Loss: 0.689301073551178\n",
      "Epoch: 1, Batch: 118, Loss: 0.6829096078872681\n",
      "Epoch: 1, Batch: 119, Loss: 0.7137569785118103\n",
      "Epoch: 1, Batch: 120, Loss: 0.6848505139350891\n",
      "Epoch: 1, Batch: 121, Loss: 0.7240488529205322\n",
      "Epoch: 1, Batch: 122, Loss: 0.6920979022979736\n",
      "Epoch: 1, Batch: 123, Loss: 0.7041601538658142\n",
      "Epoch: 1, Batch: 124, Loss: 0.6823171377182007\n",
      "Epoch: 1, Batch: 125, Loss: 0.6884211897850037\n",
      "Epoch: 2, Batch: 1, Loss: 0.7140829563140869\n",
      "Epoch: 2, Batch: 2, Loss: 0.6793611645698547\n",
      "Epoch: 2, Batch: 3, Loss: 0.6801921129226685\n",
      "Epoch: 2, Batch: 4, Loss: 0.6795952320098877\n",
      "Epoch: 2, Batch: 5, Loss: 0.691353440284729\n",
      "Epoch: 2, Batch: 6, Loss: 0.6977757215499878\n",
      "Epoch: 2, Batch: 7, Loss: 0.660290002822876\n",
      "Epoch: 2, Batch: 8, Loss: 0.6638327240943909\n",
      "Epoch: 2, Batch: 9, Loss: 0.7130175828933716\n",
      "Epoch: 2, Batch: 10, Loss: 0.688355028629303\n",
      "Epoch: 2, Batch: 11, Loss: 0.7078980803489685\n",
      "Epoch: 2, Batch: 12, Loss: 0.7657264471054077\n",
      "Epoch: 2, Batch: 13, Loss: 0.7363089323043823\n",
      "Epoch: 2, Batch: 14, Loss: 0.6849870681762695\n",
      "Epoch: 2, Batch: 15, Loss: 0.6728838682174683\n",
      "Epoch: 2, Batch: 16, Loss: 0.7253642678260803\n",
      "Epoch: 2, Batch: 17, Loss: 0.7113659977912903\n",
      "Epoch: 2, Batch: 18, Loss: 0.6905593276023865\n",
      "Epoch: 2, Batch: 19, Loss: 0.6970608830451965\n",
      "Epoch: 2, Batch: 20, Loss: 0.7052721381187439\n",
      "Epoch: 2, Batch: 21, Loss: 0.6598688364028931\n",
      "Epoch: 2, Batch: 22, Loss: 0.6503654718399048\n",
      "Epoch: 2, Batch: 23, Loss: 0.7056705951690674\n",
      "Epoch: 2, Batch: 24, Loss: 0.7130467891693115\n",
      "Epoch: 2, Batch: 25, Loss: 0.7588199973106384\n",
      "Epoch: 2, Batch: 26, Loss: 0.6862316131591797\n",
      "Epoch: 2, Batch: 27, Loss: 0.6385909914970398\n",
      "Epoch: 2, Batch: 28, Loss: 0.6155010461807251\n",
      "Epoch: 2, Batch: 29, Loss: 0.6410343647003174\n",
      "Epoch: 2, Batch: 30, Loss: 0.8053023815155029\n",
      "Epoch: 2, Batch: 31, Loss: 0.776685357093811\n",
      "Epoch: 2, Batch: 32, Loss: 0.6910870671272278\n",
      "Epoch: 2, Batch: 33, Loss: 0.7126875519752502\n",
      "Epoch: 2, Batch: 34, Loss: 0.6880162954330444\n",
      "Epoch: 2, Batch: 35, Loss: 0.6862871646881104\n",
      "Epoch: 2, Batch: 36, Loss: 0.6849042773246765\n",
      "Epoch: 2, Batch: 37, Loss: 0.6997618675231934\n",
      "Epoch: 2, Batch: 38, Loss: 0.7128078937530518\n",
      "Epoch: 2, Batch: 39, Loss: 0.6951338052749634\n",
      "Epoch: 2, Batch: 40, Loss: 0.6944458484649658\n",
      "Epoch: 2, Batch: 41, Loss: 0.7007927894592285\n",
      "Epoch: 2, Batch: 42, Loss: 0.6833571195602417\n",
      "Epoch: 2, Batch: 43, Loss: 0.6920269727706909\n",
      "Epoch: 2, Batch: 44, Loss: 0.6993082761764526\n",
      "Epoch: 2, Batch: 45, Loss: 0.6689983606338501\n",
      "Epoch: 2, Batch: 46, Loss: 0.6910595893859863\n",
      "Epoch: 2, Batch: 47, Loss: 0.6398110389709473\n",
      "Epoch: 2, Batch: 48, Loss: 0.7314630746841431\n",
      "Epoch: 2, Batch: 49, Loss: 0.6775434017181396\n",
      "Epoch: 2, Batch: 50, Loss: 0.6317701935768127\n",
      "Epoch: 2, Batch: 51, Loss: 0.7291030883789062\n",
      "Epoch: 2, Batch: 52, Loss: 0.7095245122909546\n",
      "Epoch: 2, Batch: 53, Loss: 0.706945538520813\n",
      "Epoch: 2, Batch: 54, Loss: 0.6938069462776184\n",
      "Epoch: 2, Batch: 55, Loss: 0.6877752542495728\n",
      "Epoch: 2, Batch: 56, Loss: 0.6729217171669006\n",
      "Epoch: 2, Batch: 57, Loss: 0.6850043535232544\n",
      "Epoch: 2, Batch: 58, Loss: 0.68799889087677\n",
      "Epoch: 2, Batch: 59, Loss: 0.6885790824890137\n",
      "Epoch: 2, Batch: 60, Loss: 0.8795560002326965\n",
      "Epoch: 2, Batch: 61, Loss: 0.6690173149108887\n",
      "Epoch: 2, Batch: 62, Loss: 0.685678243637085\n",
      "Epoch: 2, Batch: 63, Loss: 0.6927142143249512\n",
      "Epoch: 2, Batch: 64, Loss: 0.7294601202011108\n",
      "Epoch: 2, Batch: 65, Loss: 0.6927807331085205\n",
      "Epoch: 2, Batch: 66, Loss: 0.698533833026886\n",
      "Epoch: 2, Batch: 67, Loss: 0.6754499673843384\n",
      "Epoch: 2, Batch: 68, Loss: 0.649735689163208\n",
      "Epoch: 2, Batch: 69, Loss: 0.6930722594261169\n",
      "Epoch: 2, Batch: 70, Loss: 0.6669982075691223\n",
      "Epoch: 2, Batch: 71, Loss: 0.6220715641975403\n",
      "Epoch: 2, Batch: 72, Loss: 0.8467432856559753\n",
      "Epoch: 2, Batch: 73, Loss: 0.8946118354797363\n",
      "Epoch: 2, Batch: 74, Loss: 0.7992571592330933\n",
      "Epoch: 2, Batch: 75, Loss: 0.6918983459472656\n",
      "Epoch: 2, Batch: 76, Loss: 0.6498570442199707\n",
      "Epoch: 2, Batch: 77, Loss: 0.6973027586936951\n",
      "Epoch: 2, Batch: 78, Loss: 0.6970052719116211\n",
      "Epoch: 2, Batch: 79, Loss: 0.7132371068000793\n",
      "Epoch: 2, Batch: 80, Loss: 0.7021868228912354\n",
      "Epoch: 2, Batch: 81, Loss: 0.6591323614120483\n",
      "Epoch: 2, Batch: 82, Loss: 0.7317564487457275\n",
      "Epoch: 2, Batch: 83, Loss: 0.7273193001747131\n",
      "Epoch: 2, Batch: 84, Loss: 0.6675204038619995\n",
      "Epoch: 2, Batch: 85, Loss: 0.7036072015762329\n",
      "Epoch: 2, Batch: 86, Loss: 0.6986469030380249\n",
      "Epoch: 2, Batch: 87, Loss: 0.6934766173362732\n",
      "Epoch: 2, Batch: 88, Loss: 0.6862199306488037\n",
      "Epoch: 2, Batch: 89, Loss: 0.7047728300094604\n",
      "Epoch: 2, Batch: 90, Loss: 0.6825762987136841\n",
      "Epoch: 2, Batch: 91, Loss: 0.7043462991714478\n",
      "Epoch: 2, Batch: 92, Loss: 0.6550935506820679\n",
      "Epoch: 2, Batch: 93, Loss: 0.6247476935386658\n",
      "Epoch: 2, Batch: 94, Loss: 0.666628360748291\n",
      "Epoch: 2, Batch: 95, Loss: 0.7472954988479614\n",
      "Epoch: 2, Batch: 96, Loss: 0.6987842321395874\n",
      "Epoch: 2, Batch: 97, Loss: 0.684848427772522\n",
      "Epoch: 2, Batch: 98, Loss: 0.6633893251419067\n",
      "Epoch: 2, Batch: 99, Loss: 0.6763349771499634\n",
      "Epoch: 2, Batch: 100, Loss: 0.7024269104003906\n",
      "Epoch: 2, Batch: 101, Loss: 0.7769448161125183\n",
      "Epoch: 2, Batch: 102, Loss: 0.8544983863830566\n",
      "Epoch: 2, Batch: 103, Loss: 0.6900677680969238\n",
      "Epoch: 2, Batch: 104, Loss: 0.7162607908248901\n",
      "Epoch: 2, Batch: 105, Loss: 1.0381007194519043\n",
      "Epoch: 2, Batch: 106, Loss: 0.7027241587638855\n",
      "Epoch: 2, Batch: 107, Loss: 0.6849247813224792\n",
      "Epoch: 2, Batch: 108, Loss: 0.6889816522598267\n",
      "Epoch: 2, Batch: 109, Loss: 0.7278098464012146\n",
      "Epoch: 2, Batch: 110, Loss: 0.7200925350189209\n",
      "Epoch: 2, Batch: 111, Loss: 0.8824831247329712\n",
      "Epoch: 2, Batch: 112, Loss: 0.7025688886642456\n",
      "Epoch: 2, Batch: 113, Loss: 0.7100183963775635\n",
      "Epoch: 2, Batch: 114, Loss: 0.5813448429107666\n",
      "Epoch: 2, Batch: 115, Loss: 0.6943113207817078\n",
      "Epoch: 2, Batch: 116, Loss: 0.9278310537338257\n",
      "Epoch: 2, Batch: 117, Loss: 0.7639327049255371\n",
      "Epoch: 2, Batch: 118, Loss: 0.726942777633667\n",
      "Epoch: 2, Batch: 119, Loss: 0.7227010130882263\n",
      "Epoch: 2, Batch: 120, Loss: 0.7010354399681091\n",
      "Epoch: 2, Batch: 121, Loss: 0.7028430700302124\n",
      "Epoch: 2, Batch: 122, Loss: 0.6937398314476013\n",
      "Epoch: 2, Batch: 123, Loss: 0.6883190870285034\n",
      "Epoch: 2, Batch: 124, Loss: 0.6728091835975647\n",
      "Epoch: 2, Batch: 125, Loss: 0.6784472465515137\n",
      "Epoch: 3, Batch: 1, Loss: 0.6944477558135986\n",
      "Epoch: 3, Batch: 2, Loss: 0.7274854183197021\n",
      "Epoch: 3, Batch: 3, Loss: 0.6957281827926636\n",
      "Epoch: 3, Batch: 4, Loss: 0.6936985850334167\n",
      "Epoch: 3, Batch: 5, Loss: 0.7912695407867432\n",
      "Epoch: 3, Batch: 6, Loss: 0.6890354156494141\n",
      "Epoch: 3, Batch: 7, Loss: 0.6759133338928223\n",
      "Epoch: 3, Batch: 8, Loss: 0.6824567317962646\n",
      "Epoch: 3, Batch: 9, Loss: 0.6886386275291443\n",
      "Epoch: 3, Batch: 10, Loss: 0.6974536180496216\n",
      "Epoch: 3, Batch: 11, Loss: 0.7163748741149902\n",
      "Epoch: 3, Batch: 12, Loss: 0.7280386090278625\n",
      "Epoch: 3, Batch: 13, Loss: 0.7096133828163147\n",
      "Epoch: 3, Batch: 14, Loss: 0.7083914279937744\n",
      "Epoch: 3, Batch: 15, Loss: 0.7008606195449829\n",
      "Epoch: 3, Batch: 16, Loss: 0.6494417190551758\n",
      "Epoch: 3, Batch: 17, Loss: 0.768502950668335\n",
      "Epoch: 3, Batch: 18, Loss: 0.6417031288146973\n",
      "Epoch: 3, Batch: 19, Loss: 0.6682133674621582\n",
      "Epoch: 3, Batch: 20, Loss: 0.6828133463859558\n",
      "Epoch: 3, Batch: 21, Loss: 0.6918652057647705\n",
      "Epoch: 3, Batch: 22, Loss: 0.7275649309158325\n",
      "Epoch: 3, Batch: 23, Loss: 0.6928667426109314\n",
      "Epoch: 3, Batch: 24, Loss: 0.6576918959617615\n",
      "Epoch: 3, Batch: 25, Loss: 0.6784490346908569\n",
      "Epoch: 3, Batch: 26, Loss: 0.6797429323196411\n",
      "Epoch: 3, Batch: 27, Loss: 0.6935680508613586\n",
      "Epoch: 3, Batch: 28, Loss: 0.6647307276725769\n",
      "Epoch: 3, Batch: 29, Loss: 0.697445273399353\n",
      "Epoch: 3, Batch: 30, Loss: 0.6744848489761353\n",
      "Epoch: 3, Batch: 31, Loss: 0.7287960052490234\n",
      "Epoch: 3, Batch: 32, Loss: 0.7619533538818359\n",
      "Epoch: 3, Batch: 33, Loss: 0.6700286865234375\n",
      "Epoch: 3, Batch: 34, Loss: 0.7207987904548645\n",
      "Epoch: 3, Batch: 35, Loss: 0.7126840353012085\n",
      "Epoch: 3, Batch: 36, Loss: 0.7085378170013428\n",
      "Epoch: 3, Batch: 37, Loss: 0.6922969818115234\n",
      "Epoch: 3, Batch: 38, Loss: 0.7299113869667053\n",
      "Epoch: 3, Batch: 39, Loss: 0.7180604934692383\n",
      "Epoch: 3, Batch: 40, Loss: 0.6832295060157776\n",
      "Epoch: 3, Batch: 41, Loss: 0.6510866284370422\n",
      "Epoch: 3, Batch: 42, Loss: 0.7023502588272095\n",
      "Epoch: 3, Batch: 43, Loss: 0.7246975898742676\n",
      "Epoch: 3, Batch: 44, Loss: 0.6951247453689575\n",
      "Epoch: 3, Batch: 45, Loss: 0.6549705266952515\n",
      "Epoch: 3, Batch: 46, Loss: 0.6959657669067383\n",
      "Epoch: 3, Batch: 47, Loss: 0.691450834274292\n",
      "Epoch: 3, Batch: 48, Loss: 0.7030900716781616\n",
      "Epoch: 3, Batch: 49, Loss: 0.7053064107894897\n",
      "Epoch: 3, Batch: 50, Loss: 0.7015833854675293\n",
      "Epoch: 3, Batch: 51, Loss: 0.693690836429596\n",
      "Epoch: 3, Batch: 52, Loss: 0.7097027897834778\n",
      "Epoch: 3, Batch: 53, Loss: 0.7393249273300171\n",
      "Epoch: 3, Batch: 54, Loss: 0.7093546390533447\n",
      "Epoch: 3, Batch: 55, Loss: 0.6544972062110901\n",
      "Epoch: 3, Batch: 56, Loss: 0.7146431803703308\n",
      "Epoch: 3, Batch: 57, Loss: 0.6722277998924255\n",
      "Epoch: 3, Batch: 58, Loss: 0.6599134802818298\n",
      "Epoch: 3, Batch: 59, Loss: 0.6576213836669922\n",
      "Epoch: 3, Batch: 60, Loss: 0.6633405685424805\n",
      "Epoch: 3, Batch: 61, Loss: 0.6889406442642212\n",
      "Epoch: 3, Batch: 62, Loss: 0.716200590133667\n",
      "Epoch: 3, Batch: 63, Loss: 0.7125753164291382\n",
      "Epoch: 3, Batch: 64, Loss: 0.6911484003067017\n",
      "Epoch: 3, Batch: 65, Loss: 0.665519654750824\n",
      "Epoch: 3, Batch: 66, Loss: 0.6138924360275269\n",
      "Epoch: 3, Batch: 67, Loss: 0.6551370620727539\n",
      "Epoch: 3, Batch: 68, Loss: 0.7476924061775208\n",
      "Epoch: 3, Batch: 69, Loss: 0.7113509178161621\n",
      "Epoch: 3, Batch: 70, Loss: 0.6651577949523926\n",
      "Epoch: 3, Batch: 71, Loss: 0.7173686623573303\n",
      "Epoch: 3, Batch: 72, Loss: 0.7292217016220093\n",
      "Epoch: 3, Batch: 73, Loss: 0.7451884150505066\n",
      "Epoch: 3, Batch: 74, Loss: 0.6836044192314148\n",
      "Epoch: 3, Batch: 75, Loss: 0.6882738471031189\n",
      "Epoch: 3, Batch: 76, Loss: 0.7085002064704895\n",
      "Epoch: 3, Batch: 77, Loss: 0.6860165596008301\n",
      "Epoch: 3, Batch: 78, Loss: 0.6894891262054443\n",
      "Epoch: 3, Batch: 79, Loss: 0.6869298815727234\n",
      "Epoch: 3, Batch: 80, Loss: 0.6921885013580322\n",
      "Epoch: 3, Batch: 81, Loss: 0.6982225775718689\n",
      "Epoch: 3, Batch: 82, Loss: 0.6880853772163391\n",
      "Epoch: 3, Batch: 83, Loss: 0.6930190920829773\n",
      "Epoch: 3, Batch: 84, Loss: 0.6880208849906921\n",
      "Epoch: 3, Batch: 85, Loss: 0.6743083596229553\n",
      "Epoch: 3, Batch: 86, Loss: 0.6988612413406372\n",
      "Epoch: 3, Batch: 87, Loss: 0.7253630757331848\n",
      "Epoch: 3, Batch: 88, Loss: 0.7058632373809814\n",
      "Epoch: 3, Batch: 89, Loss: 0.7177592515945435\n",
      "Epoch: 3, Batch: 90, Loss: 0.6862276196479797\n",
      "Epoch: 3, Batch: 91, Loss: 0.6873573064804077\n",
      "Epoch: 3, Batch: 92, Loss: 0.7266448140144348\n",
      "Epoch: 3, Batch: 93, Loss: 0.7017192244529724\n",
      "Epoch: 3, Batch: 94, Loss: 0.7079604864120483\n",
      "Epoch: 3, Batch: 95, Loss: 0.6920019388198853\n",
      "Epoch: 3, Batch: 96, Loss: 0.6948347091674805\n",
      "Epoch: 3, Batch: 97, Loss: 0.6987804174423218\n",
      "Epoch: 3, Batch: 98, Loss: 0.6876352429389954\n",
      "Epoch: 3, Batch: 99, Loss: 0.6887295246124268\n",
      "Epoch: 3, Batch: 100, Loss: 0.6837171316146851\n",
      "Epoch: 3, Batch: 101, Loss: 0.6973404884338379\n",
      "Epoch: 3, Batch: 102, Loss: 0.6764667630195618\n",
      "Epoch: 3, Batch: 103, Loss: 0.696433424949646\n",
      "Epoch: 3, Batch: 104, Loss: 0.7215683460235596\n",
      "Epoch: 3, Batch: 105, Loss: 0.6744356751441956\n",
      "Epoch: 3, Batch: 106, Loss: 0.7230318188667297\n",
      "Epoch: 3, Batch: 107, Loss: 0.6219306588172913\n",
      "Epoch: 3, Batch: 108, Loss: 0.6989126205444336\n",
      "Epoch: 3, Batch: 109, Loss: 0.729368269443512\n",
      "Epoch: 3, Batch: 110, Loss: 0.6851413249969482\n",
      "Epoch: 3, Batch: 111, Loss: 0.7147918343544006\n",
      "Epoch: 3, Batch: 112, Loss: 0.7279732823371887\n",
      "Epoch: 3, Batch: 113, Loss: 0.6976743936538696\n",
      "Epoch: 3, Batch: 114, Loss: 0.6486996412277222\n",
      "Epoch: 3, Batch: 115, Loss: 0.661444365978241\n",
      "Epoch: 3, Batch: 116, Loss: 0.7457696795463562\n",
      "Epoch: 3, Batch: 117, Loss: 0.6859158873558044\n",
      "Epoch: 3, Batch: 118, Loss: 0.7074752449989319\n",
      "Epoch: 3, Batch: 119, Loss: 0.6856017112731934\n",
      "Epoch: 3, Batch: 120, Loss: 0.7044433355331421\n",
      "Epoch: 3, Batch: 121, Loss: 0.7040114402770996\n",
      "Epoch: 3, Batch: 122, Loss: 0.6536548733711243\n",
      "Epoch: 3, Batch: 123, Loss: 0.702277660369873\n",
      "Epoch: 3, Batch: 124, Loss: 0.6947866082191467\n",
      "Epoch: 3, Batch: 125, Loss: 0.6826260089874268\n",
      "Epoch: 4, Batch: 1, Loss: 0.7104056477546692\n",
      "Epoch: 4, Batch: 2, Loss: 0.7010558843612671\n",
      "Epoch: 4, Batch: 3, Loss: 0.6805123090744019\n",
      "Epoch: 4, Batch: 4, Loss: 0.6820659637451172\n",
      "Epoch: 4, Batch: 5, Loss: 0.7099740505218506\n",
      "Epoch: 4, Batch: 6, Loss: 0.7074259519577026\n",
      "Epoch: 4, Batch: 7, Loss: 0.7034211754798889\n",
      "Epoch: 4, Batch: 8, Loss: 0.6954889297485352\n",
      "Epoch: 4, Batch: 9, Loss: 0.6894505023956299\n",
      "Epoch: 4, Batch: 10, Loss: 0.695022702217102\n",
      "Epoch: 4, Batch: 11, Loss: 0.6912014484405518\n",
      "Epoch: 4, Batch: 12, Loss: 0.6924819350242615\n",
      "Epoch: 4, Batch: 13, Loss: 0.6923935413360596\n",
      "Epoch: 4, Batch: 14, Loss: 0.6952568888664246\n",
      "Epoch: 4, Batch: 15, Loss: 0.6925461292266846\n",
      "Epoch: 4, Batch: 16, Loss: 0.6938801407814026\n",
      "Epoch: 4, Batch: 17, Loss: 0.6922805905342102\n",
      "Epoch: 4, Batch: 18, Loss: 0.6955695748329163\n",
      "Epoch: 4, Batch: 19, Loss: 0.6923537850379944\n",
      "Epoch: 4, Batch: 20, Loss: 0.6874131560325623\n",
      "Epoch: 4, Batch: 21, Loss: 0.7029712200164795\n",
      "Epoch: 4, Batch: 22, Loss: 0.690338134765625\n",
      "Epoch: 4, Batch: 23, Loss: 0.6995148658752441\n",
      "Epoch: 4, Batch: 24, Loss: 0.691489040851593\n",
      "Epoch: 4, Batch: 25, Loss: 0.6923702955245972\n",
      "Epoch: 4, Batch: 26, Loss: 0.6936807632446289\n",
      "Epoch: 4, Batch: 27, Loss: 0.6910349726676941\n",
      "Epoch: 4, Batch: 28, Loss: 0.6940560340881348\n",
      "Epoch: 4, Batch: 29, Loss: 0.6921836137771606\n",
      "Epoch: 4, Batch: 30, Loss: 0.6909568905830383\n",
      "Epoch: 4, Batch: 31, Loss: 0.6932823657989502\n",
      "Epoch: 4, Batch: 32, Loss: 0.6894297003746033\n",
      "Epoch: 4, Batch: 33, Loss: 0.6970000267028809\n",
      "Epoch: 4, Batch: 34, Loss: 0.6943830251693726\n",
      "Epoch: 4, Batch: 35, Loss: 0.6971399784088135\n",
      "Epoch: 4, Batch: 36, Loss: 0.6883773803710938\n",
      "Epoch: 4, Batch: 37, Loss: 0.6904897689819336\n",
      "Epoch: 4, Batch: 38, Loss: 0.6879100799560547\n",
      "Epoch: 4, Batch: 39, Loss: 0.6908546090126038\n",
      "Epoch: 4, Batch: 40, Loss: 0.6890678405761719\n",
      "Epoch: 4, Batch: 41, Loss: 0.691535472869873\n",
      "Epoch: 4, Batch: 42, Loss: 0.7147444486618042\n",
      "Epoch: 4, Batch: 43, Loss: 0.6815828680992126\n",
      "Epoch: 4, Batch: 44, Loss: 0.681869387626648\n",
      "Epoch: 4, Batch: 45, Loss: 0.6811030507087708\n",
      "Epoch: 4, Batch: 46, Loss: 0.6947513222694397\n",
      "Epoch: 4, Batch: 47, Loss: 0.7207865715026855\n",
      "Epoch: 4, Batch: 48, Loss: 0.7110396027565002\n",
      "Epoch: 4, Batch: 49, Loss: 0.6952323913574219\n",
      "Epoch: 4, Batch: 50, Loss: 0.6677847504615784\n",
      "Epoch: 4, Batch: 51, Loss: 0.694959282875061\n",
      "Epoch: 4, Batch: 52, Loss: 0.695026159286499\n",
      "Epoch: 4, Batch: 53, Loss: 0.6791505813598633\n",
      "Epoch: 4, Batch: 54, Loss: 0.6949123740196228\n",
      "Epoch: 4, Batch: 55, Loss: 0.6667535305023193\n",
      "Epoch: 4, Batch: 56, Loss: 0.6789926290512085\n",
      "Epoch: 4, Batch: 57, Loss: 0.6784106492996216\n",
      "Epoch: 4, Batch: 58, Loss: 0.6860732436180115\n",
      "Epoch: 4, Batch: 59, Loss: 0.6745553612709045\n",
      "Epoch: 4, Batch: 60, Loss: 0.684086263179779\n",
      "Epoch: 4, Batch: 61, Loss: 0.6740533113479614\n",
      "Epoch: 4, Batch: 62, Loss: 0.7575896382331848\n",
      "Epoch: 4, Batch: 63, Loss: 0.671654224395752\n",
      "Epoch: 4, Batch: 64, Loss: 0.6697716116905212\n",
      "Epoch: 4, Batch: 65, Loss: 0.7008295059204102\n",
      "Epoch: 4, Batch: 66, Loss: 0.7149955034255981\n",
      "Epoch: 4, Batch: 67, Loss: 0.7304703593254089\n",
      "Epoch: 4, Batch: 68, Loss: 0.6574354767799377\n",
      "Epoch: 4, Batch: 69, Loss: 0.686053991317749\n",
      "Epoch: 4, Batch: 70, Loss: 0.6423057317733765\n",
      "Epoch: 4, Batch: 71, Loss: 0.685123085975647\n",
      "Epoch: 4, Batch: 72, Loss: 0.6247171759605408\n",
      "Epoch: 4, Batch: 73, Loss: 0.7009551525115967\n",
      "Epoch: 4, Batch: 74, Loss: 0.6861135363578796\n",
      "Epoch: 4, Batch: 75, Loss: 0.6518505811691284\n",
      "Epoch: 4, Batch: 76, Loss: 0.7555471658706665\n",
      "Epoch: 4, Batch: 77, Loss: 0.7571591138839722\n",
      "Epoch: 4, Batch: 78, Loss: 0.718875527381897\n",
      "Epoch: 4, Batch: 79, Loss: 0.6698668599128723\n",
      "Epoch: 4, Batch: 80, Loss: 0.653483510017395\n",
      "Epoch: 4, Batch: 81, Loss: 0.6698096394538879\n",
      "Epoch: 4, Batch: 82, Loss: 0.6700794696807861\n",
      "Epoch: 4, Batch: 83, Loss: 0.6856902837753296\n",
      "Epoch: 4, Batch: 84, Loss: 0.669197142124176\n",
      "Epoch: 4, Batch: 85, Loss: 0.653161883354187\n",
      "Epoch: 4, Batch: 86, Loss: 0.7158231735229492\n",
      "Epoch: 4, Batch: 87, Loss: 0.685782253742218\n",
      "Epoch: 4, Batch: 88, Loss: 0.6695476770401001\n",
      "Epoch: 4, Batch: 89, Loss: 0.6540283560752869\n",
      "Epoch: 4, Batch: 90, Loss: 0.6504023671150208\n",
      "Epoch: 4, Batch: 91, Loss: 0.6672614812850952\n",
      "Epoch: 4, Batch: 92, Loss: 0.7430184483528137\n",
      "Epoch: 4, Batch: 93, Loss: 0.8000017404556274\n",
      "Epoch: 4, Batch: 94, Loss: 0.703311562538147\n",
      "Epoch: 4, Batch: 95, Loss: 0.7191101312637329\n",
      "Epoch: 4, Batch: 96, Loss: 0.7196651697158813\n",
      "Epoch: 4, Batch: 97, Loss: 0.6566178202629089\n",
      "Epoch: 4, Batch: 98, Loss: 0.6698330640792847\n",
      "Epoch: 4, Batch: 99, Loss: 0.633700966835022\n",
      "Epoch: 4, Batch: 100, Loss: 0.6841862797737122\n",
      "Epoch: 4, Batch: 101, Loss: 0.6443302035331726\n",
      "Epoch: 4, Batch: 102, Loss: 0.6848317384719849\n",
      "Epoch: 4, Batch: 103, Loss: 0.7154105305671692\n",
      "Epoch: 4, Batch: 104, Loss: 0.6863754987716675\n",
      "Epoch: 4, Batch: 105, Loss: 0.7007770538330078\n",
      "Epoch: 4, Batch: 106, Loss: 0.671561062335968\n",
      "Epoch: 4, Batch: 107, Loss: 0.6989904642105103\n",
      "Epoch: 4, Batch: 108, Loss: 0.7282688617706299\n",
      "Epoch: 4, Batch: 109, Loss: 0.7141664028167725\n",
      "Epoch: 4, Batch: 110, Loss: 0.6715399622917175\n",
      "Epoch: 4, Batch: 111, Loss: 0.6968249678611755\n",
      "Epoch: 4, Batch: 112, Loss: 0.7003181576728821\n",
      "Epoch: 4, Batch: 113, Loss: 0.7360138893127441\n",
      "Epoch: 4, Batch: 114, Loss: 0.7076005935668945\n",
      "Epoch: 4, Batch: 115, Loss: 0.6756975650787354\n",
      "Epoch: 4, Batch: 116, Loss: 0.6670752167701721\n",
      "Epoch: 4, Batch: 117, Loss: 0.7138658165931702\n",
      "Epoch: 4, Batch: 118, Loss: 0.7032152414321899\n",
      "Epoch: 4, Batch: 119, Loss: 0.6953948140144348\n",
      "Epoch: 4, Batch: 120, Loss: 0.6807568073272705\n",
      "Epoch: 4, Batch: 121, Loss: 0.7021944522857666\n",
      "Epoch: 4, Batch: 122, Loss: 0.6777757406234741\n",
      "Epoch: 4, Batch: 123, Loss: 0.691999077796936\n",
      "Epoch: 4, Batch: 124, Loss: 0.689694881439209\n",
      "Epoch: 4, Batch: 125, Loss: 0.7260476350784302\n",
      "Epoch: 5, Batch: 1, Loss: 0.6987407803535461\n",
      "Epoch: 5, Batch: 2, Loss: 0.6954396963119507\n",
      "Epoch: 5, Batch: 3, Loss: 0.6804189085960388\n",
      "Epoch: 5, Batch: 4, Loss: 0.6972076892852783\n",
      "Epoch: 5, Batch: 5, Loss: 0.6863686442375183\n",
      "Epoch: 5, Batch: 6, Loss: 0.6836864948272705\n",
      "Epoch: 5, Batch: 7, Loss: 0.697600245475769\n",
      "Epoch: 5, Batch: 8, Loss: 0.6943875551223755\n",
      "Epoch: 5, Batch: 9, Loss: 0.6894406080245972\n",
      "Epoch: 5, Batch: 10, Loss: 0.6844870448112488\n",
      "Epoch: 5, Batch: 11, Loss: 0.68778395652771\n",
      "Epoch: 5, Batch: 12, Loss: 0.6892064213752747\n",
      "Epoch: 5, Batch: 13, Loss: 0.6884002089500427\n",
      "Epoch: 5, Batch: 14, Loss: 0.6967822909355164\n",
      "Epoch: 5, Batch: 15, Loss: 0.6749746799468994\n",
      "Epoch: 5, Batch: 16, Loss: 0.700535774230957\n",
      "Epoch: 5, Batch: 17, Loss: 0.6650935411453247\n",
      "Epoch: 5, Batch: 18, Loss: 0.6781870126724243\n",
      "Epoch: 5, Batch: 19, Loss: 0.6526739597320557\n",
      "Epoch: 5, Batch: 20, Loss: 0.6955860257148743\n",
      "Epoch: 5, Batch: 21, Loss: 0.6882863640785217\n",
      "Epoch: 5, Batch: 22, Loss: 0.6846712827682495\n",
      "Epoch: 5, Batch: 23, Loss: 0.6721834540367126\n",
      "Epoch: 5, Batch: 24, Loss: 0.7111433148384094\n",
      "Epoch: 5, Batch: 25, Loss: 0.6718500852584839\n",
      "Epoch: 5, Batch: 26, Loss: 0.7172977924346924\n",
      "Epoch: 5, Batch: 27, Loss: 0.7365657687187195\n",
      "Epoch: 5, Batch: 28, Loss: 0.6824676990509033\n",
      "Epoch: 5, Batch: 29, Loss: 0.6190930604934692\n",
      "Epoch: 5, Batch: 30, Loss: 0.752878725528717\n",
      "Epoch: 5, Batch: 31, Loss: 0.7157576680183411\n",
      "Epoch: 5, Batch: 32, Loss: 0.688461422920227\n",
      "Epoch: 5, Batch: 33, Loss: 0.7152478098869324\n",
      "Epoch: 5, Batch: 34, Loss: 0.7320439219474792\n",
      "Epoch: 5, Batch: 35, Loss: 0.7104426026344299\n",
      "Epoch: 5, Batch: 36, Loss: 0.6720961332321167\n",
      "Epoch: 5, Batch: 37, Loss: 0.6603460907936096\n",
      "Epoch: 5, Batch: 38, Loss: 0.7095445990562439\n",
      "Epoch: 5, Batch: 39, Loss: 0.6709818840026855\n",
      "Epoch: 5, Batch: 40, Loss: 0.72771155834198\n",
      "Epoch: 5, Batch: 41, Loss: 0.6948444843292236\n",
      "Epoch: 5, Batch: 42, Loss: 0.7150523662567139\n",
      "Epoch: 5, Batch: 43, Loss: 0.6957418322563171\n",
      "Epoch: 5, Batch: 44, Loss: 0.6993273496627808\n",
      "Epoch: 5, Batch: 45, Loss: 0.6930227279663086\n",
      "Epoch: 5, Batch: 46, Loss: 0.6971139311790466\n",
      "Epoch: 5, Batch: 47, Loss: 0.6928001642227173\n",
      "Epoch: 5, Batch: 48, Loss: 0.6993154883384705\n",
      "Epoch: 5, Batch: 49, Loss: 0.6899349689483643\n",
      "Epoch: 5, Batch: 50, Loss: 0.6900690793991089\n",
      "Epoch: 5, Batch: 51, Loss: 0.6849173307418823\n",
      "Epoch: 5, Batch: 52, Loss: 0.6946970224380493\n",
      "Epoch: 5, Batch: 53, Loss: 0.6888077259063721\n",
      "Epoch: 5, Batch: 54, Loss: 0.6990744471549988\n",
      "Epoch: 5, Batch: 55, Loss: 0.6933388710021973\n",
      "Epoch: 5, Batch: 56, Loss: 0.6934072971343994\n",
      "Epoch: 5, Batch: 57, Loss: 0.6933743953704834\n",
      "Epoch: 5, Batch: 58, Loss: 0.6935309767723083\n",
      "Epoch: 5, Batch: 59, Loss: 0.703454852104187\n",
      "Epoch: 5, Batch: 60, Loss: 0.6884561777114868\n",
      "Epoch: 5, Batch: 61, Loss: 0.6971588134765625\n",
      "Epoch: 5, Batch: 62, Loss: 0.6980845928192139\n",
      "Epoch: 5, Batch: 63, Loss: 0.6928411722183228\n",
      "Epoch: 5, Batch: 64, Loss: 0.6950392723083496\n",
      "Epoch: 5, Batch: 65, Loss: 0.6949703693389893\n",
      "Epoch: 5, Batch: 66, Loss: 0.6906874179840088\n",
      "Epoch: 5, Batch: 67, Loss: 0.6964924335479736\n",
      "Epoch: 5, Batch: 68, Loss: 0.6921262145042419\n",
      "Epoch: 5, Batch: 69, Loss: 0.6924647092819214\n",
      "Epoch: 5, Batch: 70, Loss: 0.6931779980659485\n",
      "Epoch: 5, Batch: 71, Loss: 0.6940760612487793\n",
      "Epoch: 5, Batch: 72, Loss: 0.6912472248077393\n",
      "Epoch: 5, Batch: 73, Loss: 0.6965634822845459\n",
      "Epoch: 5, Batch: 74, Loss: 0.6961095929145813\n",
      "Epoch: 5, Batch: 75, Loss: 0.6954948902130127\n",
      "Epoch: 5, Batch: 76, Loss: 0.6947863101959229\n",
      "Epoch: 5, Batch: 77, Loss: 0.6931387186050415\n",
      "Epoch: 5, Batch: 78, Loss: 0.6960811614990234\n",
      "Epoch: 5, Batch: 79, Loss: 0.6940192580223083\n",
      "Epoch: 5, Batch: 80, Loss: 0.6899294853210449\n",
      "Epoch: 5, Batch: 81, Loss: 0.6931605935096741\n",
      "Epoch: 5, Batch: 82, Loss: 0.6942167282104492\n",
      "Epoch: 5, Batch: 83, Loss: 0.6944332122802734\n",
      "Epoch: 5, Batch: 84, Loss: 0.6943176984786987\n",
      "Epoch: 5, Batch: 85, Loss: 0.6887211799621582\n",
      "Epoch: 5, Batch: 86, Loss: 0.689031720161438\n",
      "Epoch: 5, Batch: 87, Loss: 0.6978912949562073\n",
      "Epoch: 5, Batch: 88, Loss: 0.6909146904945374\n",
      "Epoch: 5, Batch: 89, Loss: 0.69309401512146\n",
      "Epoch: 5, Batch: 90, Loss: 0.683800458908081\n",
      "Epoch: 5, Batch: 91, Loss: 0.6892050504684448\n",
      "Epoch: 5, Batch: 92, Loss: 0.6935180425643921\n",
      "Epoch: 5, Batch: 93, Loss: 0.6947970390319824\n",
      "Epoch: 5, Batch: 94, Loss: 0.6926015019416809\n",
      "Epoch: 5, Batch: 95, Loss: 0.6833743453025818\n",
      "Epoch: 5, Batch: 96, Loss: 0.6863116025924683\n",
      "Epoch: 5, Batch: 97, Loss: 0.6955636739730835\n",
      "Epoch: 5, Batch: 98, Loss: 0.7011669874191284\n",
      "Epoch: 5, Batch: 99, Loss: 0.7066097855567932\n",
      "Epoch: 5, Batch: 100, Loss: 0.6819316148757935\n",
      "Epoch: 5, Batch: 101, Loss: 0.6919389963150024\n",
      "Epoch: 5, Batch: 102, Loss: 0.6982345581054688\n",
      "Epoch: 5, Batch: 103, Loss: 0.6807840466499329\n",
      "Epoch: 5, Batch: 104, Loss: 0.7037851214408875\n",
      "Epoch: 5, Batch: 105, Loss: 0.7209308743476868\n",
      "Epoch: 5, Batch: 106, Loss: 0.7064740657806396\n",
      "Epoch: 5, Batch: 107, Loss: 0.7080925107002258\n",
      "Epoch: 5, Batch: 108, Loss: 0.6942875981330872\n",
      "Epoch: 5, Batch: 109, Loss: 0.6821284294128418\n",
      "Epoch: 5, Batch: 110, Loss: 0.692868173122406\n",
      "Epoch: 5, Batch: 111, Loss: 0.6791530251502991\n",
      "Epoch: 5, Batch: 112, Loss: 0.6929926872253418\n",
      "Epoch: 5, Batch: 113, Loss: 0.6908581852912903\n",
      "Epoch: 5, Batch: 114, Loss: 0.6800860166549683\n",
      "Epoch: 5, Batch: 115, Loss: 0.7022452354431152\n",
      "Epoch: 5, Batch: 116, Loss: 0.7041636109352112\n",
      "Epoch: 5, Batch: 117, Loss: 0.6903903484344482\n",
      "Epoch: 5, Batch: 118, Loss: 0.6861658096313477\n",
      "Epoch: 5, Batch: 119, Loss: 0.6913529634475708\n",
      "Epoch: 5, Batch: 120, Loss: 0.6891488432884216\n",
      "Epoch: 5, Batch: 121, Loss: 0.6715463995933533\n",
      "Epoch: 5, Batch: 122, Loss: 0.697697103023529\n",
      "Epoch: 5, Batch: 123, Loss: 0.6907139420509338\n",
      "Epoch: 5, Batch: 124, Loss: 0.6986773014068604\n",
      "Epoch: 5, Batch: 125, Loss: 0.6869024038314819\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
